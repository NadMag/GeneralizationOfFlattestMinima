#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{amsmath}
\usepackage{amsthm}
% Necessary Commands:
\usepackage{autobreak}
\usepackage{relsize}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

%referencing layouts
\newref{lem}{refcmd={\emph{Lemma\,\ref{#1}}}}
\newref{cor}{refcmd={\emph{Corollary\,\ref{#1}}}}
\newref{prop}{refcmd={\emph{Proposition\,\ref{#1}}}}
\newref{thm}{refcmd={\emph{Theorem\,\ref{#1}}}}
%\providecommand{\propautorefname}{Proposition}

\providecommand\phantomsection{}

%Double struck 0, 1
\usepackage{bbold}

\usepackage{chngcntr}
\counterwithin*{section}{part}
\end_preamble
\use_default_options true
\begin_modules
todonotes
pdfform
theorems-ams-bytype
theorems-ams-extended-bytype
enumitem
theorems-sec-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex8
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex-natbib
\cite_engine_type authoryear
\biblio_style plainnat
\biblatex_bibstyle verbose
\biblatex_citestyle authoryear
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation 0bp
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand input
filename "MacrosEnglish.lyx"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
date{}
\end_layout

\begin_layout Plain Layout


\backslash
author{Nadav Magar, Artiom Makhlin
\backslash

\backslash
[0.3cm]{
\backslash
small Supervised by: Noam Razin, Nadav Cohen}}
\end_layout

\end_inset


\end_layout

\begin_layout Title
On Flat Minima in Linear Neural Networks
\end_layout

\begin_layout Abstract
Empirical evidence suggests that for a variety of overparameterized models,
 flat minima—those around which the loss grows slowly— consistently correlate
 with low generalization error.
 Furthermore, it has been shown that (stochastic) gradient descent exhibits
 implicit bias towards such minima.
 This phenomena makes minima flatness an appealing measurement in the study
 of generalization.
 Nevertheless, it is still an open theoretical problem why and under which
 circumstances flatness is connected to generalization.
 In this work we study this connection by focusing on the simplest class
 of overparameterized models: linear neural networks.
 First, we analyze the properties of flat minima, extending known results
 for the case of an infinitude of solutions.
 We then prove, that under reasonable assumptions, all solutions must have
 the same flatness
\shape italic
, for any local flatness measure
\shape default
.
 Our results show that in the setting of overparameterized linear regression,
 flatness does not differentiate between solutions - making it ineffective
 of explaining generalization in linear models.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Flatness and Generalization 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Flatness-and-generalization"

\end_inset


\end_layout

\begin_layout Standard
Recent advances in machine learning have relied on training highly overparameter
ized models, notably deep neural networks, to fit natural data.
 In this setting the number of learnable weights far exceeds that of the
 training samples, thereby resulting in models that achieve near-zero training
 error.
 Indeed, many modern neural networks can easily memorize the training data
 and have the capacity to readily overfit 
\begin_inset CommandInset citation
LatexCommand citep
key "stillRequiresRethinkingGen"
literal "false"

\end_inset

.
 In contrast to classical learning paradigms and theory, that advocate the
 use of small hypothesis classes and explicit regularization to limit overfittin
g, such overparameterized models exhibit a remarkably small gap between
 training and test performance.
 Recent work suggests ubiquity of the “double descent” phenomenon 
\begin_inset CommandInset citation
LatexCommand citep
key "Belkin19Reconciling,stillRequiresRethinkingGen"
literal "false"

\end_inset

, wherein significant overparameterization actually improves generalization.
 Furthermore generalizing solutions are found by simple optimization techniques
 such as stochastic gradient descent (SGD), even without explicit regularization
 
\begin_inset CommandInset citation
LatexCommand citep
key "stillRequiresRethinkingGen"
literal "false"

\end_inset

.
 The common view is that gradient-based optimization induces an implicit
 regularization — a tendency to fit data with predictors of low complexity
 measure (see, e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "Neyshabur17ImplicitThesis,gunasekar2017implicit,arora2019implicit,razin2020implicit,razin2021implicit,razin2022implicit"
literal "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
The flatness of solutions — roughly meaning the rate at which the loss grows
 around them— is one such complexity measure that has been extensively studied
 both theoretically 
\begin_inset CommandInset citation
LatexCommand citep
key "hochreiter1997flat,dziugaite2017nonvacuosPACBayes,valle-perez2018deep,chaudhari2019entropy"
literal "false"

\end_inset

 and empirically 
\begin_inset CommandInset citation
LatexCommand citep
key "keskar2016large,foret2021sharpnessaware"
literal "false"

\end_inset

.
 Notably, 
\begin_inset CommandInset citation
LatexCommand citep
key "Jiang2020Fantastic"
literal "false"

\end_inset

 conducted a large-scale empirical study and found that flatness-based measures
 correlate better with generalization than alternatives like weight norms,
 margin-, and optimization-based measures.
 Nevertheless why and under circumstances this correlation holds remains
 an open problem.
 In particular 
\begin_inset CommandInset citation
LatexCommand citep
key "dinh2017sharp"
literal "false"

\end_inset

 showed that reparameterizations of ReLU neural networks can change simple
 measures of flatness, without affecting the model's represented function
 and generalization, suggesting that such measures may capture superfluous
 correlation rather then casual connections 
\begin_inset CommandInset citation
LatexCommand citep
key "Jiang2020Fantastic"
literal "false"

\end_inset

.
 However 
\begin_inset CommandInset citation
LatexCommand citep
key "Jiang2020Fantastic"
literal "false"

\end_inset

 found empirical evidence that other flatness measures had the best causal
 relationship with generalization in comparison with other measures, and
 
\begin_inset CommandInset citation
LatexCommand citep
key "petzka2021relativeFlatness"
literal "false"

\end_inset

 proposed a relative-flatness measure that is reparameterization invariant.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
Another fact that makes flatness an appealing measurement in the study of
 generalization, is that SGD has implicit bias towards flat solutions, namely
 by its inability to stably converge to sharp minima 
\begin_inset CommandInset citation
LatexCommand citep
key "jastrzkebski2017three,wu2018sgd,simsekli2019tail"
literal "false"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique"
literal "false"

\end_inset

 have showed that under certain conditions GD and its continuous counterpart
 gradient flow can only converge to flattest minima in simple settings.
 This bias may play a part in SGD's success in finding generalizing solutions,
 supporting the conjecture that there exists a significant connection between
 the two notions.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename assets/Keskar_et_al.png
	scale 60
	rotateOrigin centerBaseline

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citep
key "keskar2016large"
literal "false"

\end_inset

 A Conceptual Sketch of Flat and Sharp Minima.
 The Y-axis indicates value of the loss function and the X-axis the weight
 space.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Subsection
Linear Neural Networks
\begin_inset CommandInset label
LatexCommand label
name "subsec:Linear-Neural-Networks"

\end_inset


\end_layout

\begin_layout Standard
To simplify our research of generalization and flatness, we focus on a specific
 neural network model called Linear neural networks.
 Linear neural networks (LNN) are fully connected networks with no neuron
 activation.
 The end-to-end function of a deep linear network can always be rewritten
 as a shallow network, thus such networks do not gain expressive power from
 increased depth, and will perform poorly on complex real world problems.
 While it lacks important aspects of modern neural network architectures,
 it is useful to appeal to the simple case of linear models to see if there
 are parallel insights that can help us better understand neural networks.
 Indeed under reasonable assumptions their loss landscape is non-convex
 
\begin_inset CommandInset citation
LatexCommand citep
key "saxe2013exact"
literal "false"

\end_inset

 and there exist "bad" saddle points (where the Hessian has no negative
 eigenvalue) 
\begin_inset CommandInset citation
LatexCommand citep
key "kawaguchi2016deep"
literal "false"

\end_inset

.
 Furthermore the training dynamics of gradient-flow on LNN show that increased
 depth may implicitly accelerate optimization 
\begin_inset CommandInset citation
LatexCommand citep
key "arora2018optimization"
literal "false"

\end_inset

, exhibit incremental rank learning 
\begin_inset CommandInset citation
LatexCommand citep
key "Arora2019Convrg"
literal "false"

\end_inset

 and is implicitly biased towards minimal norm-like solutions 
\begin_inset CommandInset citation
LatexCommand citep
key "yun2021a"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Main Results and Outline of the Paper
\end_layout

\begin_layout Standard
The remainder of the paper is organized as follows.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Non-Singular-Problem-Setting"
plural "false"
caps "false"
noprefix "false"

\end_inset

 describes the non-singular problem setting where a single end-to-end solution
 exist.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Balancedness-of-Flattest"
plural "false"
caps "false"
noprefix "false"

\end_inset

 the notion of 
\emph on
balancedness
\emph default
 of flattest solutions, specifically it is shown that not all flattest solutions
 are balanced (as defined in 
\begin_inset CommandInset citation
LatexCommand citet
key "arora2018optimization"
literal "false"

\end_inset

).
 This inequivalence is shown under two different measures of minima sharpness:
 maximal Hessian eigenvalue and Hessian trace.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:singular-case"
plural "false"
caps "false"
noprefix "false"

\end_inset

 extends the analysis of flattest minima, presented in 
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique"
literal "false"

\end_inset

, to setting where an infinitude of solutions exist.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Generalization-of-Flattest"
plural "false"
caps "false"
noprefix "false"

\end_inset

 discusses the relationship between flatness and generalization.
 In this section we provide a proof that generalization is indifferent to
 flatness, in our settings.
 We prove this result under two different settings: whitened data for the
 maximal Hessian eigenvalue measure, and full rank label data for 
\emph on
any
\emph default
 local flatness measure.
\end_layout

\begin_layout Section
Problem Setting
\end_layout

\begin_layout Standard
Consider the case of linear regression: given a sample 
\begin_inset Formula $S=\left(x^{(i)},y^{(i)}\right)_{i=1}^{N}\subseteq\left(\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}}\right),S\stackrel{i.i.d}{\sim}{\scriptstyle D^{N}}$
\end_inset

.
 Denote 
\begin_inset Formula $X=\left(x^{(1)},x^{(2)},...,x^{(N)}\right)\in\mathbb{R}^{d_{x}\times N},Y=\left(y^{(1)},y^{(2)},...,y^{(N)}\right)\in\mathbb{R}^{d_{y}\times N}$
\end_inset

, our objective is to learn a linear transformation 
\begin_inset Formula $f:\mathbb{R}^{d_{x}}\rightarrow\mathbb{R}^{d_{y}}$
\end_inset

 corresponding to a matrix 
\begin_inset Formula $W\in\mathbb{R}^{d_{y}\times d_{x}}$
\end_inset

, that minimizes the empirical (quadratic) loss: 
\begin_inset Formula $L(W)=\left\Vert WX-Y\right\Vert _{F}^{2}$
\end_inset

.
 We study the overparameterized regime, in which 
\begin_inset Formula $W$
\end_inset

 is decomposed as a product of matrices (LNN): 
\begin_inset Formula $W_{i}\in\mathbb{R}^{d_{i}\times d_{i-1}},i\in\left[m\right]$
\end_inset

 where 
\begin_inset Formula $d_{0}:=d_{x},d_{N}:=d_{y}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Denote 
\begin_inset Formula $W_{1:m}:=\prod_{i=1}^{m}W_{i}$
\end_inset

 ,the overparameterized objective is defined by: 
\begin_inset Formula 
\begin{equation}
\phi(W_{1},W_{2},...,W_{m})=L(W_{1:m})\label{eq:overparamloss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The set of global minima of 
\begin_inset Formula $\phi$
\end_inset

 is given by:
\begin_inset Formula 
\begin{equation}
\Omega=\left\{ (W_{1},W_{2},...,W_{m})\in\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}\ \Bigm\vert W_{1:m}\in\argmin_{T\in\mathbb{R}^{d_{y}\times d_{x}}}L(T)\right\} 
\end{equation}

\end_inset


\end_layout

\begin_layout Definition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Minima Flatness
\end_layout

\end_inset

Denote the vectorization of the network's parameters 
\begin_inset Formula $(W_{1},W_{2},...,W_{m})$
\end_inset

.
 Let 
\begin_inset Formula $\omega:\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}\rightarrow\mathbb{R}$
\end_inset

 be some sharpness measure, then the set of 
\emph on
flattest 
\emph default
global minima is defined to be:
\begin_inset Formula 
\begin{equation}
\Omega_{0}=\argmin_{w\in\Omega}\omega(W_{1},W_{2},...,W_{m})\label{eq:flattestmin}
\end{equation}

\end_inset


\end_layout

\begin_layout Definition
Where in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:flattestmin"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we have used a slight abuse of notation by identifying 
\begin_inset Formula $(W_{1},W_{2},...,W_{m})$
\end_inset

 with 
\begin_inset Formula $w$
\end_inset

 for brevity.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Balancedness of Flattest Solutions 
\begin_inset CommandInset label
LatexCommand label
name "sec:Balancedness-of-Flattest"

\end_inset


\end_layout

\begin_layout Standard
As previously discussed SGD shows implicit bias towards flat solutions,
 and is able to find generalizing solutions.
 Furthermore a variety of optimization algorithms that explicitly bias towards
 flat solutions achieve impressive performance 
\begin_inset CommandInset citation
LatexCommand citep
key "izmailov2018averaging,chaudhari2019entropy,foret2021sharpnessaware"
literal "false"

\end_inset

.
 In light of this phenomena, one would expect that flat solutions are in
 some sense regular, residing in benign regions where optimization algorithms
 perform well.
\end_layout

\begin_layout Standard
Several works have showed a relationship between measures of flatness and
 those of 
\shape italic
balancedness
\shape default
 —a notion of scaling and alignment between consecutive weight matrices—
 
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique,ding2022flat"
literal "false"

\end_inset

.
 A particular definition of balancedness that is of interest in our setting
 was proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "arora2018optimization"
literal "false"

\end_inset

:
\end_layout

\begin_layout Definition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Balancedness
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "def:balancedness"

\end_inset

For 
\begin_inset Formula $\delta\ge0$
\end_inset

 we say that the weight matrices 
\begin_inset Formula $\kaliseries W,m$
\end_inset

 are 
\begin_inset Formula $\delta-$
\end_inset

balanced if:
\begin_inset Formula 
\begin{equation}
\left\Vert W_{j+1}^{T}W_{j+1}-W_{j}W_{j}^{T}\right\Vert _{F}\le\delta\quad,\forall j\in[m-1]
\end{equation}

\end_inset


\end_layout

\begin_layout Definition
If 
\begin_inset Formula $\kaliseries W,N$
\end_inset

 are 0-balanced we simply say that they are balanced.
 Let us denote the set of balanced weight matrices by:
\begin_inset Formula 
\begin{equation}
\mathcal{B}=\left\{ (W_{1},W_{2},...,W_{m})\in\prod_{j=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}\ \Bigm\vert\ W_{j+1}^{T}W_{j+1}=W_{j}W_{j}^{T}\quad,\forall j\in[m-1]\right\} 
\end{equation}

\end_inset


\end_layout

\begin_layout Definition
\begin_inset VSpace defskip
\end_inset

When we optimize the model, we seek to minimize some loss function.
 To do this, a common pratctice is to use some kind of Gradient Descent
 algorithm.
\end_layout

\begin_layout Standard
For the theoritcal approach, we use the continuous equivalent to Gradient
 Descent - the Gradient Flow algorithm, wich works as follows:
\end_layout

\begin_layout Definition
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Gradient Flow
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "def:gf"

\end_inset

 Let 
\begin_inset Formula $\mathbb{W}$
\end_inset

 be the linear space of weights of the model, and 
\begin_inset Formula $f:\mathbb{W}\rightarrow\mathbb{R}$
\end_inset

 be a smooth loss function.
 Gradient flow(or steepest descent curve) is a smoothh curve 
\begin_inset Formula $w:\mathbb{R}\rightarrow\mathbb{W}$
\end_inset

 such that:
\end_layout

\begin_layout Definition
\begin_inset Formula 
\[
\dot{w}\left(t\right)=-\nabla f\left(w\left(t\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
A useful fact is that this notion of balancedness is conserved by GF throughout
 optimization, formally:
\end_layout

\begin_layout Proposition
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Arora2019Convrg"
literal "false"

\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "prop:balancedness"

\end_inset

Assume the weight matrices 
\begin_inset Formula $\kaliseries W,m$
\end_inset

 are initialized to be 
\begin_inset Formula $\delta-\mathrm{balanced}$
\end_inset

: 
\begin_inset Formula 
\[
\left\Vert W_{j+1}^{T}(0)W_{j+1}(0)-W_{j}(0)W_{j}^{T}(0)\right\Vert _{F}\le\delta,\quad\forall j\in[m-1]
\]

\end_inset

 this property is conserved throughout optimization:
\begin_inset Formula 
\[
\left\Vert W_{j+1}^{T}(t)W_{j+1}(t)-W_{j}(t)W_{j}^{T}(t)\right\Vert _{F}\le\delta,\quad\kaliForall j\in[m-1],\ t\ge0
\]

\end_inset


\end_layout

\begin_layout Standard
We seek to refine the relationship between 
\begin_inset CommandInset ref
LatexCommand eqref
reference "prop:balancedness"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and two common measures of flatness: maximal Hessian eigenvalue, and the
 Hessian trace.
\end_layout

\begin_layout Subsection
Maximal Hessian Eigenvalue
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $H_{w}$
\end_inset

 be the Hessian matrix of 
\begin_inset Formula $\phi$
\end_inset

 at the point 
\begin_inset Formula $\left(\kaliseries W,m\right)$
\end_inset

.
 Define a sharpness measure 
\begin_inset Formula $\omega:\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}\rightarrow\mathbb{R}$
\end_inset

 by:
\begin_inset Formula 
\begin{equation}
\omega\left(\kaliseries W,m\right)=\lambda_{max}\left(H_{w}\right)\label{eq:max-hessian-eigenvalue}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Note that this measure captures the sharpness of a point in the 
\begin_inset Quotes eld
\end_inset

worst-case
\begin_inset Quotes erd
\end_inset

 sense, and is the factor affecting stable convergence of GD and SGD 
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique,nar2018step"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In particular they studied the case where a single end-to-end solution exists:
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

label=
\backslash
bfseries{C1}
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "assumption:nonsingular"

\end_inset

 The data's ambient dimension 
\begin_inset Formula $d_{x}$
\end_inset

 is smaller then the number of training samples 
\begin_inset Formula $N$
\end_inset

, and we have at least 
\begin_inset Formula $d_{x}$
\end_inset

 linearly independent samples.
\end_layout

\begin_layout Standard
In this case the unique solution can be written as 
\begin_inset Formula $f^{\ast}(x)=Tx$
\end_inset

 where: 
\begin_inset Formula 
\begin{equation}
T=\hat{\Sigma}_{yx}\hat{\Sigma}_{x}^{-1}:=YX^{T}\left(XX^{T}\right)^{-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We will deonte the the SVD of 
\begin_inset Formula $T$
\end_inset

 by:
\begin_inset Formula 
\begin{equation}
T=USV^{T}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique"
literal "false"

\end_inset

 showed that under the assumptions:
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

label=
\backslash
bfseries{A
\backslash
arabic{enumi}}
\end_layout

\end_inset


\end_layout

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "assumption:capacity"

\end_inset

The network has the capacity to implement any linear function from 
\begin_inset Formula $\mathbb{R}^{d_{x}}$
\end_inset

 to 
\begin_inset Formula $\mathbb{R}^{d_{y}}$
\end_inset

: 
\begin_inset Formula $\min_{i\in\left[m\right]}\{d_{i}\}\ge\min\{d_{x},d_{y}\}.$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "assumption:white"

\end_inset

The data is white, namely 
\begin_inset Formula $\hat{\Sigma}_{x}=I_{d_{x}}$
\end_inset

.
\end_layout

\begin_layout Standard
The following sufficient condition for minima flatness holds:
\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Sufficient Condition, 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Mulayoff2020Unique"
literal "false"

\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:sufficient"

\end_inset

Assume 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:nonsingular"
plural "false"
caps "false"
noprefix "false"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:white"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 If a solution 
\begin_inset Formula $w\in\Omega$
\end_inset

 satisfies 
\begin_inset Formula 
\[
\sigma_{\max}(W_{k})=\left(\sigma_{\max}\left(T\right)\right)^{\frac{1}{m}}
\]

\end_inset

 for all 
\begin_inset Formula $k\in[m]$
\end_inset

, then necessarily 
\begin_inset Formula $w\in\Omega_{0}.$
\end_inset


\end_layout

\begin_layout Standard
Using this fact it can be shown that GF starting from a balanced initialization
 can only converge to flattest solutions.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:balancedness"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can see that any balanced solution 
\begin_inset Formula $w\in\Omega\cap\mathcal{B}$
\end_inset

 is in fact a flattest minima:
\begin_inset Formula 
\begin{equation}
\Omega\cap\mathcal{B}\subseteq\Omega_{0}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
One may wonder if the reverse implication holds:
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align center

\shape italic
Are all flattest solutions balanced? 
\end_layout

\begin_layout Standard
The following proposition shows that this is not the case:
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:FlatUnbalanced"

\end_inset

Assume 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:nonsingular"
plural "false"
caps "false"
noprefix "false"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:white"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and that 
\begin_inset Formula $2\le\min\{d_{x},d_{y}\},m$
\end_inset

.
 Unless 
\begin_inset Formula $\sigma_{\min}\left(T\right)=\sigma_{\max}\left(T\right)$
\end_inset

, then there exists an unbalanced flattest solution, namely: 
\begin_inset Formula 
\[
\Omega_{0}\nsubseteq\Omega\cap\mathcal{B}
\]

\end_inset


\end_layout

\begin_layout Proof
\begin_inset ERT
status open

\begin_layout Plain Layout

[Proof sketch (proof in 
\backslash
ref{subsec:proof-flat-unbalanced})]
\end_layout

\end_inset

 We find a flattest unbalanced solutions by 
\begin_inset Quotes eld
\end_inset

perturbing
\begin_inset Quotes erd
\end_inset

 the singular values of consecutive weight matrices of a balanced solution:
 
\begin_inset Formula $\left(\kaliseries W,m\right)\in\Omega_{0}\cap\mathcal{B}$
\end_inset

, without changing its end-to-end function.
 
\end_layout

\begin_layout Proof
As 
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique"
literal "false"

\end_inset

 state, it is intractable to derive a closed form expression for 
\begin_inset Formula $\lambda_{\max}\left(H_{w}\right)$
\end_inset

 at an arbitrary minimum point, we therefore perturb the solution to meet
 the conditions of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sufficient"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Remark*
Unfortunately the authors could not find such a perturbation for the case
 of 
\begin_inset Formula $\sigma_{\min}\left(T\right)=\sigma_{\max}\left(T\right)$
\end_inset

 or find a more lenient sufficient condition.
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\begin_inset VSpace defskip
\end_inset

Hessian Trace
\end_layout

\begin_layout Standard
The proof of 
\begin_inset Formula $\text{\prettyref{prop:FlatUnbalanced}}$
\end_inset

relied on the fact that the maximal eigenvalue of the Hessian of 
\begin_inset Formula $\phi$
\end_inset

 is unaffected by a small perturbation of the overparameterization non-maximal
 singular values.
 The proof shows the 
\begin_inset Quotes eld
\end_inset

worst case
\begin_inset Quotes erd
\end_inset

 nature of this measure 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:max-hessian-eigenvalue"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We now study a different measure, that depends on all of the overparameterizati
on singular values: the Hessian trace.
 Formally, define:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\omega\left(\kaliseries W,m\right)=Trace\left(H_{w}\right)\label{eq:sharp_def_trace}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $w$
\end_inset

 is defined as 
\begin_inset Formula $w:=Vec(W_{1},W_{2},...,W_{m})$
\end_inset

 such that 
\begin_inset Formula $W_{m}\cdot...\cdot W_{1}=T$
\end_inset

.
\end_layout

\begin_layout Standard
Next we revise the results of 
\begin_inset CommandInset citation
LatexCommand citet
key "Mulayoff2020Unique"
literal "false"

\end_inset

 for the trace measure.
\end_layout

\begin_layout Subsubsection
Sharpness of a general solution
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:hes-trace"

\end_inset

Assuming 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:nonsingular"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:white"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the trace of the Hessian for a solution is:
\end_layout

\begin_layout Lemma
\begin_inset Formula 
\begin{equation}
Trace(H_{w})=2\sum_{k=1}^{m}||W_{k-1:1}||_{F}^{2}\cdot||W_{m:k+1}||_{F}^{2}\label{eq:hes-trace}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
\begin_inset ERT
status open

\begin_layout Plain Layout

[Proof sketch (proof in 
\backslash
ref{subsec:proof-of-hes-trace})]
\end_layout

\end_inset

 
\end_layout

\begin_layout Proof
We apply the trace function to the result from 
\begin_inset CommandInset citation
LatexCommand citet
key "Mulayoff2020Unique"
literal "false"

\end_inset

 for the Hessian structure, and using properties of Kronecker product and
 trace, arrive to the result above.
\end_layout

\begin_layout Subsubsection
Sharpness of a balanced solution
\end_layout

\begin_layout Standard
Now, we discuss the case of a balanced solution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\forall i\in[m].W_{i}W_{i}^{T}=W_{i+1}^{T}W_{i+1}
\]

\end_inset


\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:bal-hes-trace"

\end_inset

For a balnced solution, it holds that: 
\begin_inset Formula $||\prod_{i=k}^{n}W_{i}||_{F}^{2}=||S^{\frac{n-k+1}{m}}||_{F}^{2}$
\end_inset

 where 
\begin_inset Formula $W_{m:1}=T=USV^{T}$
\end_inset

is the SVD of the End-to-End solution.
\end_layout

\begin_layout Proof
From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "subsec:proof-flat-unbalanced"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we know the following:
\begin_inset Formula 
\[
U_{k}=V_{k+1},S_{k}=S_{k+1}
\]

\end_inset

 Thus we get that:
\begin_inset Formula 
\[
\prod_{i=k}^{n}W_{i}=W_{n}W_{n-1}...W_{k+1}W_{k}=\mysvd n\mysvd{n-1}...\mysvd{k+1}\mysvd k=U_{n}S_{k}^{n-k+1}V_{k}^{T}
\]

\end_inset

 Comparing it with the end-to-end solution we get:
\begin_inset Formula 
\[
U_{m}S_{1}^{m}V_{1}^{T}=W_{m:1}=T=USV^{T}=>\forall i\in[m].S_{i}=S^{\frac{1}{m}}
\]

\end_inset

 which yields:
\begin_inset Formula 
\[
||\prod_{i=k}^{n}W_{i}||_{F}^{2}=||U_{n}S_{k}^{n-k+1}V_{k}^{T}||_{F}^{2}=||S^{\frac{n-k+1}{m}}||_{F}^{2}
\]

\end_inset


\end_layout

\begin_layout Proof
By using 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:hes-trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

 with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hes-trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we get the following result for balanced solutions:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{equation}
Trace(H_{w})=2\sum_{k=1}^{m}||S^{\frac{k-1}{m}}||_{F}^{2}\cdot||S^{\frac{m-k}{m}}||_{F}^{2}\label{eq:balanced_trace}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Is balancedness equal to flattest solution?
\end_layout

\begin_layout Standard
After revisiting 
\begin_inset CommandInset citation
LatexCommand citet
key "Mulayoff2020Unique"
literal "false"

\end_inset

 with our new definition for the sharpness of a solution, we can start and
 investigate if balancedness equates to flatness of the solution.
\end_layout

\begin_layout Standard
We split this to 2 cases: the 2-Layer case, and the general m-layer Case.
 As we will show, there is difference in the results, which stems from the
 scaling of some elements in the equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hes-trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
2-Layer case
\end_layout

\begin_layout Paragraph
Non-Square Case
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
In the non-square case, we have an issue of scaling: the trace has a different
 scaling for each norm, thus providing us a way to find a counter example.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:non-square-case"

\end_inset

 Assume 
\begin_inset Formula $T\in\mathbb{R}^{d_{X}\times d_{Y}}$
\end_inset

 and 
\begin_inset Formula $d_{X}\neq d_{Y}$
\end_inset

.
 Then there exists a solution with lower sharpness than any balanced solution.
\end_layout

\begin_layout Proof
\begin_inset ERT
status open

\begin_layout Plain Layout

[Proof sketch (proof in 
\backslash
ref{subsec:proof-of-non-square-case})]
\end_layout

\end_inset


\end_layout

\begin_layout Proof
We bring a counter-example which has lower sharpness than the balanced solution.
 As all balanced solutions have the same sharpness (by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:balanced_trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we conclude that any balanced solution is not the flattest, and any flattest
 solution is not balanced.
\end_layout

\begin_layout Paragraph
Square Case
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
In the square case, all the elements in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hes-trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are scales with the same constant - the dimension.
 Using this, we prove that a solution is flattest if and only if the solution
 is balanced.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:2-layer-case"

\end_inset

Assume that 
\begin_inset Formula $T=W_{2}W_{1}$
\end_inset

 and 
\begin_inset Formula $T\in\mathbb{R}^{d\times d}.$
\end_inset

 Then a solution is flattest if and only if the solution is balanced.
\end_layout

\begin_layout Proof
\begin_inset ERT
status open

\begin_layout Plain Layout

[Proof sketch (proof in 
\backslash
ref{subsec:proof-of-2-layer-case})]
\end_layout

\end_inset


\end_layout

\begin_layout Proof
First we show that balanced solution is always a flattest solution.
\end_layout

\begin_layout Proof
To show this we show that minimizing the trace of the hessian in the 2-Layer
 case for square end-to-end matrix is equal to the nuclear norm of the end-to-en
d solution.
\end_layout

\begin_layout Proof
To show the other direction, that any flattest solution is indeed balanced,
 we define a new function that achieves a local minima at the solution.
 By differentiating this function and equating the derivative to 0, we get
 the balancedness criterion defined in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "def:balancedness"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
m-Layer Case
\end_layout

\begin_layout Standard
As we saw before, the non-square case the statement is already invalid.
 Thus we will focus here only on the square case.
\end_layout

\begin_layout Standard
We will show now that for the m-Layer case for 
\begin_inset Formula $m\geq3$
\end_inset

 balancedness does not imply flattest solution, or vice versa.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:m-layer-case"

\end_inset

 Assume 
\begin_inset Formula $T\in\mathbb{R}^{d\times d}$
\end_inset

 .
 Then for 
\begin_inset Formula $m>2$
\end_inset

, there exists a solution 
\begin_inset Formula $w=\left(W_{m},...W_{1}\right)$
\end_inset

 with lower sharpness than any balanced solution.
\end_layout

\begin_layout Proof
\begin_inset ERT
status open

\begin_layout Plain Layout

[Proof sketch (proof in 
\backslash
ref{subsec:proof-of-m-layer-case})]
\end_layout

\end_inset


\end_layout

\begin_layout Proof
We use a similar approach as in the proof of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:non-square-case"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We build a counter example, and show that there exist a value for the singular
 values of the end-to-end matrix, such that any balanced solution will have
 a higher sharpness value than our counter example.
\end_layout

\begin_layout Standard
In conclusion, we see that even the new definition of sharpness (
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:sharp_def_trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

) is not enough to achieve balancedness for the flattest solutions, apart
 from the simple case of 2-layers.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Analysis of the Multiple End-To-End Soultions Case 
\begin_inset CommandInset label
LatexCommand label
name "sec:singular-case"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Mulayoff2020Unique"
literal "false"

\end_inset

 intuitively argued that in the singular case any end-to-end solution has
 the same flatness, thus they could assume that a single global minima exits
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:nonsingular"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We prove their claim in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Generalization-of-Flattest"
plural "false"
caps "false"
noprefix "false"

\end_inset

, but first we extend their analysis to the case where 
\begin_inset Formula $n<d_{x}$
\end_inset

 and multiple end-to-end solutions exist.
 The results carry over naturally to the singular case, and would later
 allow us to prove the aforementioned claim.
\end_layout

\begin_layout Subsection
Singular Problem Setting
\end_layout

\begin_layout Standard
Define the set of end-to-end global minimizers by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\Psi=\left\{ T\in\mathbb{R}^{d_{y}\times d_{x}}\ \Bigm\vert\ \in\argmin_{W\in\mathbb{R}^{d_{y}\times d_{x}}}L(W)\right\} 
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
For a single end-to-end solution 
\begin_inset Formula $T\in\mathbb{R}^{d_{y}\times d_{x}}$
\end_inset

 we denote the set of overparameterizations as a LNN by:
\begin_inset Formula 
\begin{equation}
\Omega(T)=\left\{ (W_{1},W_{2},...,W_{m})\in\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}\ \Bigm\vert\ W_{1:m}=T\right\} 
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Define the sharpness of an overparameterization 
\begin_inset Formula $\omega:\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}\rightarrow\mathbb{R}$
\end_inset

, and of sharpness a solution 
\begin_inset Formula $\rho:\mathbb{R}^{d_{y}\times d_{x}}\rightarrow\mathbb{R}$
\end_inset

 respectively:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\omega(W_{1},W_{2},...,W_{m})=\lambda_{\mathrm{\max}}\left(H_{w}\right),\quad\rho(T)=\min_{(W_{1},W_{2},...,W_{m})\in\Omega(T)}\omega(W_{1},W_{2},...,W_{m})\label{def:e2e-flatness}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\lambda_{\mathrm{\max}}\left(H_{w}\right)$
\end_inset

 is continuous in 
\begin_inset Formula $w$
\end_inset

 and that 
\begin_inset Formula $\Omega(T)$
\end_inset

 is closed in 
\begin_inset Formula $\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}$
\end_inset

 so 
\begin_inset Formula $\rho$
\end_inset

 is well-defined.
\end_layout

\begin_layout Standard
We denote the set of flattest overparameterizations for a solution 
\begin_inset Formula $T$
\end_inset

 by:
\begin_inset Formula 
\[
\Omega_{0}(T)=\argmin_{(W_{1},W_{2},...,W_{m})\in\Omega(T)}\omega(W_{1},W_{2},...,W_{m})
\]

\end_inset


\end_layout

\begin_layout Standard
To study the connection between flatness and generalization one must consider
 the singular case where more then one end-to-end solutions exits (
\begin_inset Formula $N<d_{x}$
\end_inset

 and the mapping is realizable) .
 In this case all such solutions are given by:
\begin_inset Formula 
\begin{equation}
\Psi=\left\{ YX^{+}+K\left(I_{d_{x}}-XX^{+}\right)\Bigm\vert K\in\mathbb{R}^{d_{y}\times d_{x}}\right\} \label{eq:singular-solutions}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\cdot^{+}$
\end_inset

 is the Moore-Penrose pseudo-inverse.
\end_layout

\begin_layout Standard
Denote the set of flattest solutions by:
\begin_inset Formula 
\begin{equation}
\Psi_{0}=\argmin_{T\in\Psi}\rho(T)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We begin by examining a simplified version of the singular case, we modify
 assumption 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:white"
plural "false"
caps "false"
noprefix "false"

\end_inset

:
\end_layout

\begin_layout Enumerate
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

start=2, label=
\backslash
bfseries{A
\backslash
arabic{enumi}}
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "assumption:pseudo-white"

\end_inset

 The data is pseudo-white, namely 
\begin_inset Formula $\hat{\Sigma}_{x}=\begin{pmatrix}I_{N} & \mathbb{0}_{N\times\left(d_{x}-N\right)}\\
\mathbb{0}_{\left(d_{x}-N\right)\times N} & \mathbb{0}_{d_{x}-N}
\end{pmatrix}$
\end_inset

.
\end_layout

\begin_layout Remark*
We note that this assumption may be unreasonable for study of generalization.
 Beside its inapplicability to modern practices, whitening is known to potential
ly impede generalization —mainly due to re-scaling of feature variances
 
\begin_inset CommandInset citation
LatexCommand citep
key "wadia20whitening"
literal "false"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Mulayoff2020Unique"
literal "false"

\end_inset

 conjectured that analogous result hold in the general setting, there however,
 there seems to be no known closed form expression for 
\begin_inset Formula $\lambda_{\mathrm{\max}}\left(H_{w}\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Extending the Analysis to the Multiple End-To-End Soultions Case
\end_layout

\begin_layout Standard
We now extend 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Mulayoff2020Unique"
literal "false"

\end_inset

 results to the singular setting described in the previous section.
\end_layout

\begin_layout Standard
First, note that lemma 2 (Hessian Structure) holds for any 
\begin_inset Formula $(W_{1},W_{2},...,W_{m})\in\Omega(T)$
\end_inset

, 
\begin_inset Formula $T\in\Psi$
\end_inset

.
\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
Minimal Solution Sharpness
\end_layout

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "thm:min-sharpness-singular"

\end_inset

Assume 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

, then the minimal solution sharpness is given by 
\begin_inset Formula $\min_{T\in\Psi}\rho(T)=2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}$
\end_inset

, i.e.: 
\begin_inset Formula 
\[
\Psi_{0}=\left\{ T\in\Psi\ \Bigm\vert\ \in\rho(T)=2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
The proof follows the scheme of the singular case 
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique"
literal "false"

\end_inset

:
\end_layout

\begin_layout Enumerate
First we derive a uniform lower bound on the sharpness of an end-to-end
 solution.
\end_layout

\begin_layout Enumerate
We show a solution which achieves this bound.
\end_layout

\begin_layout Remark*
In the previous case of a single end-to-end soltuion, the minimal solution
 sharpness was 
\begin_inset Formula $2m\left(\sigma_{\max}\left(T\right)\right)^{2\left(1-\frac{1}{m}\right)}$
\end_inset

 where 
\begin_inset Formula $T$
\end_inset

 is the end-to-end solution.
\end_layout

\begin_layout Remark*
We see a similarity to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "thm:min-sharpness-singular"
plural "false"
caps "false"
noprefix "false"

\end_inset

, as in the multiple solution case each solution has the following form:
 
\begin_inset Formula 
\[
T=YX^{+}+\left(I-XX^{+}\right)
\]

\end_inset


\end_layout

\begin_layout Remark*
And indeed, if 
\begin_inset Formula $X$
\end_inset

 is a full rank we are in the single solution case, and 
\begin_inset Formula $T=YX^{+}=YX^{T}\left(XX^{T}\right)^{-1}$
\end_inset

 which gives us exactly the same value as in the theorem.
\end_layout

\begin_layout Lemma
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
Uniform Lower Bound
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lemma:unif-lb"

\end_inset

Let 
\begin_inset Formula $T\in\Psi$
\end_inset

 be a solution, define 
\begin_inset Formula $\nu_{T}:\mathbb{R}^{d_{y}\times d_{x}}\rightarrow\mathbb{R}_{\ge0}$
\end_inset

 by: 
\begin_inset Formula 
\[
\nu_{T}\left(B\right)=2m\left\Vert \left(B\hat{\Sigma}_{x}^{\frac{1}{2}}T^{T}\right)^{m-1}B\hat{\Sigma}_{x}^{\frac{1}{2}}\right\Vert _{2}^{\frac{2}{m}}
\]

\end_inset

Then the following holds:
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

label=
\backslash
Roman{enumi}.
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Formula $\max_{\left\Vert B\right\Vert _{F}=1}\nu_{T}\left(B\right)\le\rho(T)$
\end_inset

 .
 
\end_layout

\begin_layout Enumerate
For the special case of 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $\forall T_{1},T_{2}\in\Psi.\max_{\left\Vert B\right\Vert _{F}=1}\nu_{T_{1}}\left(B\right)=\max_{\left\Vert B\right\Vert _{F}=1}\nu_{T_{2}}\left(B\right)=\rho(T)$
\end_inset

, namely:
\begin_inset Formula 
\[
\rho(T)\ge2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}\ ,\quad\forall T\in\Psi
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Proof
\begin_inset ERT
status open

\begin_layout Plain Layout

[Proof sketch (proof in 
\backslash
ref{subsec:proof-flat-unbalanced})]
\end_layout

\end_inset

 The first part of the lemma proved in 
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique"
literal "false"

\end_inset

.
 Note that 
\begin_inset Formula $\max_{\left\Vert B\right\Vert _{F}=1}\nu_{T}\left(B\right)\le\rho(T)$
\end_inset

 uniformly bounds all overparameterizations 
\begin_inset Formula $w\in\Omega(T)$
\end_inset

.
\end_layout

\begin_layout Proof
To see how the dependence on a specific end-to-end solution is dropped in
 the section part, notice that for a solution 
\begin_inset Formula $T=YX^{+}+K\left(I_{d_{x}}-XX^{+}\right)$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\hat{\Sigma}_{x}^{\frac{1}{2}}T^{T} & =XX^{T}\left(\left(X^{T}\right)^{+}Y^{T}+\left(I_{d_{x}}-\left(X^{T}\right)^{+}X^{T}\right)K^{T}\right)\\
 & =X{\color{gray}\underset{=I_{N}}{\underbrace{{\normalcolor X^{T}\left(X^{T}\right)^{+}}}}}Y^{T}+X{\color{gray}\underset{=\mathbb{0}_{N\times d_{x}}}{\underbrace{{\normalcolor X^{T}\left(I_{d_{x}}-\left(X^{T}\right)^{+}X^{T}\right)}}}}K^{T}\\
 & =YX^{+}
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Where the third equality is due to to the properties of the pseudo-inverse
 and that by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\begin_inset Formula $X^{T}$
\end_inset

 has linearly-independent columns, thus 
\begin_inset Formula $\left(X^{T}\right)^{+}$
\end_inset

 is a left inverse.
\end_layout

\begin_layout Standard
Next we show that there exist a solution that achieves this bound:
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:lower-bound-achieved"

\end_inset

Assume 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 There exists 
\begin_inset Formula $T\in\Psi$
\end_inset

 such that 
\begin_inset Formula $\rho(T)=2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}$
\end_inset

.
\end_layout

\begin_layout Standard

\shape italic
See proof in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:proof-of-lem-acheived"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
Using the necessary condition of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:min-sharpness-singular"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we are able to derive a sufficient condition analogous to the non-singular
 case (
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sufficient"
plural "false"
caps "false"
noprefix "false"

\end_inset

):
\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
Sufficient Condition
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "thm:sufficient-singular"

\end_inset

Assume 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 If an overparameterization 
\begin_inset Formula $w\in\varTheta(T)$
\end_inset

 of a solution 
\begin_inset Formula $T\in\Psi$
\end_inset

 satisfies 
\begin_inset Formula $\sigma_{\max}(W_{k})=\left(\sigma_{\max}\left(YX^{T}\right)\right)^{\frac{1}{m}}$
\end_inset

 for all 
\begin_inset Formula $k\in[m]$
\end_inset

, then necessarily 
\begin_inset Formula $w\in\Omega_{0}(T)$
\end_inset

 and 
\begin_inset Formula $T\in\Psi_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
The proof of this theorem is analogous to the non-singular case in 
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique"
literal "false"

\end_inset

, by using 
\begin_inset CommandInset ref
LatexCommand eqref
reference "thm:min-sharpness-singular"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Subsection
Generalization of Flattest Solutions 
\begin_inset CommandInset label
LatexCommand label
name "sec:Generalization-of-Flattest"

\end_inset


\end_layout

\begin_layout Subsubsection
Motivation: Simple Linear Regression
\end_layout

\begin_layout Standard
As discussed in sections 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Flatness-and-generalization"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Linear-Neural-Networks"
plural "false"
caps "false"
noprefix "false"

\end_inset

 it is not necessarily easy to understand the source of generalization even
 for linear models.
 Before studying the overparameterized setting, let us take a step back
 an examine the case of simple underdetermined linear regression with the
 quadratic loss: 
\begin_inset Formula 
\[
L(W)=\left\Vert WX-Y\right\Vert _{F}^{2}
\]

\end_inset

In this setting the Hessian matrix of the loss function is given by: 
\begin_inset Formula 
\[
H_{L}(W)=2\cdot I_{d_{y}\times d_{x}}\left(XX^{T}\right)\odot I_{d_{x}\times d_{y}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\odot$
\end_inset

 denotes the Kronecker product.
\end_layout

\begin_layout Standard
Notice that the Hessian is independent of the choice of 
\begin_inset Formula $W$
\end_inset

, hence any sharpness definition that depends only on it, cannot correlate
 with generalization 
\begin_inset CommandInset label
LatexCommand label
name "subsec:reg-regression-indf"

\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "choromanska14surface"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Mulayoff2020Unique"
literal "false"

\end_inset

 claimed that a similar phenomena holds in the overparameterized setting
 as-well, namely any end-to-end solution has the same flatness, formally:
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:indifference"

\end_inset

Assume 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 All end-to-end solutions have the same flatness, namely: 
\begin_inset Formula 
\[
\Psi=\Psi_{0}
\]

\end_inset


\end_layout

\begin_layout Standard
Note that this claim is not trivial in the overparameterized setting, as
 the Hessian is location dependent, and a closed form expression for its
 top eigenvalue seems intractable.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:-hessian-structure"

\end_inset


\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
Hessian Structure 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Mulayoff2020Unique"
literal "false"

\end_inset


\end_layout

\end_inset

 Assume 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 If 
\begin_inset Formula $w\in\Omega\left(T\right)$
\end_inset

 and 
\begin_inset Formula $T\in\Psi$
\end_inset

 is a solution, then
\begin_inset Formula 
\[
H_{w}=2\Phi\Phi^{T}
\]

\end_inset


\end_layout

\begin_layout Lemma
where 
\begin_inset Formula $\Phi=\left[\kaliseries{\Phi^{T}},m\right]^{T}$
\end_inset

, with 
\begin_inset Formula 
\[
\Phi_{k}=\left(W_{k-1:1}\hat{\Sigma}_{x}^{\frac{1}{2}}\right)\odot\left(W_{m:k+1}\right)^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
The Hessian structure provides intuition for 
\begin_inset CommandInset ref
LatexCommand eqref
reference "thm:indifference"
plural "false"
caps "false"
noprefix "false"

\end_inset

: note that 
\begin_inset Formula $\Phi_{k}$
\end_inset

 is a 
\begin_inset Formula $d_{k}d_{k-1}\times d_{x}d_{y}$
\end_inset

 matrix, therefore 
\begin_inset Formula $\Phi$
\end_inset

 is a 
\begin_inset Formula $P\times d_{x}d_{y}$
\end_inset

 where 
\begin_inset Formula $P=\sum_{k=1}^{m}d_{k}d_{k-1}$
\end_inset

 is the total number of parameters in the network.
 This shows that for networks with more than one layer the Hessian at an
 end-to-end solution is rank-deficient.
 Formally, by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:-hessian-structure"
plural "false"
caps "false"
noprefix "false"

\end_inset

 if 
\begin_inset Formula $w\in\Omega(T)$
\end_inset

 and 
\begin_inset Formula $T\in\Psi$
\end_inset

 then 
\begin_inset Formula $H_{w}$
\end_inset

 is a 
\begin_inset Formula $P\times P$
\end_inset

 matrix, and 
\begin_inset Formula $rank\left(H_{w}\right)\le d_{x}d_{y}<P^{2}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Proof Outline for Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:indifference"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Standard
We begin by showing that if a point 
\begin_inset Formula $w\in\Theta(T_{1})$
\end_inset

 at the overparameterized space satisfies a certain regularity constraint
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:overparam-regularity"
plural "false"
caps "false"
noprefix "false"

\end_inset

, then for every other 
\begin_inset Formula $T_{2}\in\mathbb{R}^{d_{y}\times d_{x}}$
\end_inset

 such that 
\begin_inset Formula $\left(T_{2}-T_{1}\right)X=0$
\end_inset

, there exists a direction 
\begin_inset Formula $\bar{w}$
\end_inset

 in the overparameterized space, that connects 
\begin_inset Formula $\Theta(T_{1})$
\end_inset

 with 
\begin_inset Formula $\Theta(T_{2})$
\end_inset

, along which 
\begin_inset Formula $\phi$
\end_inset

 is constant.
\end_layout

\begin_layout Proposition
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
Invariant Directions
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "prop:invariant-directions"

\end_inset

Assuming 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption:capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 let 
\begin_inset Formula $T_{1},T_{2}\in\mathbb{R}^{d_{y}\times d_{x}}$
\end_inset

 such that 
\begin_inset Formula $\left(T_{2}-T_{1}\right)X=0$
\end_inset

.
 Assume 
\begin_inset Formula $T_{1}$
\end_inset

 admits an overparameterization 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\left(\kaliseries\bar{W},m\right)\in\varTheta(T_{1})$
\end_inset

 such that: 
\begin_inset Formula 
\begin{equation}
rank\left(\bar{W}_{2:m}\right)=d_{y}\ or\ rank\left(\bar{W}_{1:m-1}\right)=d_{x}\label{eq:overparam-regularity}
\end{equation}

\end_inset

 then
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 there exists a direction 
\begin_inset Formula $\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)\in\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}$
\end_inset

 such that: 
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

label=
\backslash
alph{enumi}
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Formula $T_{2}=\left(\bar{W}_{i}+\Delta_{i}\right)_{1:m}$
\end_inset


\end_layout

\begin_layout Enumerate
The overparameterized loss is constant along 
\begin_inset Formula $\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right),$
\end_inset

 namely: 
\begin_inset Formula 
\[
\phi\left(\left(\kaliseries\bar{W},m\right)+t\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)\right)=\phi\left(\kaliseries\bar{W},m\right)\ ,\ \forall t\in\mathbb{R}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Proof
Assume 
\begin_inset Formula $d_{y}\le d_{x}$
\end_inset

, define 
\begin_inset Formula $\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)\in\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}$
\end_inset

 by:
\begin_inset Formula 
\begin{equation}
\Delta_{1}=\left(\bar{W}_{2:m}\right)^{+}\left(T_{2}-T_{1}\right)\ ,\quad\Delta_{i}=\mathbb{0}_{d_{i}\times d_{i-1}},i\in\left\{ 2,3,...,m\right\} \label{def:dead-direction-delta}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
Notice that by construction 
\begin_inset Formula $\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)$
\end_inset

 shifts 
\begin_inset Formula $\left(\kaliseries\bar{W},m\right)$
\end_inset

 to 
\begin_inset Formula $\varTheta(T_{1}),$
\end_inset

i.e.:
\begin_inset Formula 
\[
\left(\bar{W}_{i}+\Delta_{i}\right)_{1:m}=\bar{W}_{2:m}\left(\bar{W}_{1}+\left(\bar{W}_{2:m}\right)^{+}\left(T_{2}-T_{1}\right)\right)=T_{2}
\]

\end_inset


\end_layout

\begin_layout Proof
Where the last equality holds because by assumption 
\begin_inset Formula $\bar{W}_{2:m}$
\end_inset

 has linearly-independent rows, so 
\begin_inset Formula $\left(\bar{W}_{2:m}\right)^{+}$
\end_inset

 is a right inverse.
\end_layout

\begin_layout Proof
Next we show that the loss is constant along 
\begin_inset Formula $\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\phi\left(\left(\kaliseries\bar{W},m\right)+t\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)\right) & =\left\Vert \left(\bar{W}_{i}+t\Delta_{i}\right)_{1:m}X-Y\right\Vert _{F}^{2}\\
 & =\left\Vert \bar{W}_{2:m}\left(\bar{W}_{1}+t\left(\bar{W}_{2:m}\right)^{+}\left(T_{2}-T_{1}\right)\right)X-Y\right\Vert _{F}^{2}\\
 & =\left\Vert \bar{W}_{1:m}X+t\bar{W}_{2:m}\left(\bar{W}_{2:m}\right)^{+}\left(T_{2}-T_{1}\right)X-Y\right\Vert _{F}^{2}\\
 & =\left\Vert \bar{W}_{1:m}X-Y\right\Vert _{F}^{2}\\
 & =\phi\left(\kaliseries\bar{W},m\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Where in the third equality we used the assumption 
\begin_inset Formula $\left(T_{2}-T_{1}\right)X=0$
\end_inset

.
\end_layout

\begin_layout Proof
For the case where 
\begin_inset Formula $d_{x}<d_{y}$
\end_inset

 we can similarly define:
\begin_inset Formula 
\[
\Delta_{m}=\left(\bar{W}_{1:m-1}\right)^{+}\left(T_{2}-T_{1}\right)\ ,\quad\Delta_{i}=\mathbb{0}_{d_{i}\times d_{i-1}},i\in\left[m-1\right]
\]

\end_inset


\end_layout

\begin_layout Proof
In this case 
\begin_inset Formula $W_{1:m-1}$
\end_inset

 has linearly-independent columns, so 
\begin_inset Formula $\left(W_{1:m-1}\right)^{+}$
\end_inset

 is a left inverse, the proof is analogous.
\end_layout

\begin_layout Corollary
\begin_inset CommandInset label
LatexCommand label
name "cor:dead-direction-local"

\end_inset

Under the assumptions and notations of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:invariant-directions"
plural "false"
caps "false"
noprefix "false"

\end_inset

, there exists a direction 
\begin_inset Formula $\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)\in\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}$
\end_inset

 such that:
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $T_{2}=\left(\bar{W}_{i}+\Delta_{i}\right)_{1:m}$
\end_inset


\end_layout

\begin_layout Enumerate
The overparameterized loss 
\begin_inset Formula $\phi$
\end_inset

 is locally constant along 
\begin_inset Formula $\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)$
\end_inset

.
 Formally define the translation function:
\begin_inset Formula 
\[
s_{t}:\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}\rightarrow\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}},\quad s(W_{1},W_{2},...,W_{m})=(W_{1},W_{2},...,W_{m})+t\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)
\]

\end_inset

then there exist 
\begin_inset Formula $\varepsilon>0$
\end_inset

 such that:
\begin_inset Formula 
\[
\forall t\in\mathbb{R}.\ \phi|_{B\left((\bar{W}_{1},\bar{W}_{2},...,\bar{W}_{m}),\varepsilon\right)}\equiv\phi\circ s_{t}|_{B\left((\bar{W}_{1},\bar{W}_{2},...,\bar{W}_{m}),\varepsilon\right)}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:dead-direction-local"
plural "false"
caps "false"
noprefix "false"

\end_inset

 provides an lower bounds the sharpness of 
\shape italic
all 
\emph on
solutions 
\begin_inset Formula $T\in\Psi$
\end_inset

 by the sharpness of a solution 
\begin_inset Formula $T_{0}$
\end_inset

 that admits 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:overparam-regularity"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Thus to prove the theorem, it is suffices to show the existence of a 
\emph default
flattest
\emph on
 solution 
\begin_inset Formula $T_{0}\in\Psi_{0}$
\end_inset

 that admits 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:overparam-regularity"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Remark*
Note that while 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:dead-direction-local"
plural "false"
caps "false"
noprefix "false"

\end_inset

 holds for any local definition of sharpness 
\begin_inset Formula $\omega$
\end_inset

, the existence of such a flattest solution may depend on the specific definitio
n.
 In section 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:def-agnostic-indf"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we provide a definition-agnostic proof, under different assumptions.
 
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:existence-of-fr"

\end_inset

Assuming 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 There exists 
\begin_inset Formula $T_{0}\in\Psi_{0}$
\end_inset

 that admits a flattest overparameterization 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $(W_{1},W_{2},...,W_{m})\in\varTheta_{0}(T_{0})$
\end_inset

 such that: 
\begin_inset Formula 
\[
rank\left(\bar{W}_{2:m}\right)=d_{y}\ or\ rank\left(\bar{W}_{1:m-1}\right)=d_{x}
\]

\end_inset


\end_layout

\begin_layout Proof
\begin_inset ERT
status open

\begin_layout Plain Layout

[Proof sketch (proof in 
\backslash
ref{subsec:proof-of-ex-of-frs})]
\end_layout

\end_inset

The proof follows a similar argument as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:FlatUnbalanced"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Let 
\begin_inset Formula $\left(W_{1}^{\ast},W_{2}^{\ast},...,W_{m}^{\ast}\right)$
\end_inset

 be the canonical overparameterization of 
\begin_inset Formula $T_{0}$
\end_inset

.
 We define a new overparameterization 
\begin_inset Formula $\left(W_{1},W_{2},...,W_{m}\right)$
\end_inset

 by 
\begin_inset Quotes eld
\end_inset

perturbing
\begin_inset Quotes erd
\end_inset

 the singular values of each weight matrix 
\begin_inset Formula $W_{j}^{\ast}$
\end_inset

 for 
\begin_inset Formula $2\le j\le m$
\end_inset

 into non-singularity.
 Because 
\begin_inset Formula $\left(W_{1}^{\ast},W_{2}^{\ast},...,W_{m}^{\ast}\right)$
\end_inset

 are balanced this perturbation does not change the end-to-end function.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sufficient-singular"
plural "false"
caps "false"
noprefix "false"

\end_inset

, for a sufficiently small perturbation, 
\begin_inset Formula $\left(W_{1},W_{2},...,W_{m}\right)$
\end_inset

 is a flattest overparameterization of 
\begin_inset Formula $T_{0}$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Completing the Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:indifference"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Standard
We show the case where 
\begin_inset Formula $d_{y}\le d_{x}$
\end_inset

, the case of 
\begin_inset Formula $d_{x}<d_{y}$
\end_inset

 is analogous.
 
\end_layout

\begin_layout Standard
By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:existence-of-fr"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we know that there exists 
\begin_inset Formula $T_{0}\in\Psi_{0}$
\end_inset

 that admits an overparameterization 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $(U_{1},U_{2},...,U_{m})\in\varTheta(T_{0})$
\end_inset

 such that 
\begin_inset Formula $rank\left(W_{2:m}\right)=d_{y}$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $T'\in\Psi$
\end_inset

 be a solution, and 
\begin_inset Formula $(W'_{1},W'_{2},...,W'_{m})\in\varTheta(T')$
\end_inset

 an overparameterization of it, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:dead-direction-local"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for 
\begin_inset Formula 
\[
\Delta_{1}=\left(W_{2:m}\right)^{+}\left(T_{2}-T_{1}\right)\ ,\quad\Delta_{i}=\mathbb{0}_{d_{i}\times d_{i-1}},i\in\left\{ 2,3,...,m\right\} 
\]

\end_inset

it holds that:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $(U_{1},U_{2},...,U_{m})+(\Delta_{1},\Delta_{2},...,\Delta_{m})\in\varTheta(T')$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\exists\varepsilon>0.\ \phi|_{B\left((U_{1},U_{2},...,U_{m}),\varepsilon\right)}\equiv\phi\circ s_{1}|_{B\left((\bar{W}_{1},\bar{W}_{2},...,\bar{W}_{m}),\varepsilon\right)}$
\end_inset


\end_layout

\begin_layout Standard
Because 
\begin_inset Formula $\phi$
\end_inset

 is locally identical in a neighborhood of 
\begin_inset Formula $(U_{1},U_{2},...,U_{m})$
\end_inset

 and of 
\begin_inset Formula $(U_{1}+\Delta_{1},U_{2}+\Delta_{2},...,U_{m}+\Delta_{m})$
\end_inset

, it particularly holds that: 
\begin_inset Formula 
\begin{equation}
H_{\phi}(U_{1},U_{2},...,U_{m})=H_{\phi}(U_{1}+\Delta_{1},U_{2}+\Delta_{2},...,U_{m}+\Delta_{m})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
and specifically:
\begin_inset Formula 
\[
\rho\left(T'\right)\stackrel{\ref{eq:petubation-between-e2e}}{\le}\lambda_{\max}\left(H_{\phi}(U_{1}+\Delta_{1},U_{2}+\Delta_{2},...,U_{m}+\Delta_{m})\right)=\lambda_{\mathrm{\max}}\left(H_{\phi}(U_{1},U_{2},...,U_{m})\right)=2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
Now by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sufficient-singular"
plural "false"
caps "false"
noprefix "false"

\end_inset

: 
\begin_inset Formula $2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}\le\rho\left(T'\right)$
\end_inset

, hence:
\begin_inset Formula 
\[
T'\in\Psi_{0}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:def-agnostic-indf"

\end_inset

Invariance Under Any Local Measure
\end_layout

\begin_layout Standard
As discussed in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Balancedness-of-Flattest"
plural "false"
caps "false"
noprefix "false"

\end_inset

 there are many different definitions for the sharpness of a solutions.
 
\end_layout

\begin_layout Standard
The proof of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "lem:existence-of-fr"
plural "false"
caps "false"
noprefix "false"

\end_inset

 relied on tolerance of the maximal Hessian eigenvalue to small perturbations
 of singular values, this begs the question:
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\align center

\shape italic
Is there a definition of sharpness under which not all solutions are flattest?
\end_layout

\begin_layout Standard
The following corollary of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:dead-direction-local"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows that under the assumption that 
\begin_inset Formula $d_{y}\le n\le d_{x}$
\end_inset

 and that 
\begin_inset Formula $rank(Y)=d_{y}$
\end_inset

 (as is usually the case in deep learning), any sharpness measure 
\begin_inset Formula $\omega:\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}\rightarrow\mathbb{R}$
\end_inset

 that depends only on a local neighborhood of 
\begin_inset Formula $\phi$
\end_inset

 does not correlate with generalization.
\end_layout

\begin_layout Theorem
Assume that that 
\begin_inset Formula $d_{y}\le N\le d_{x}$
\end_inset

, 
\begin_inset Formula $rank(Y)=d_{y}$
\end_inset

 and 
\begin_inset Formula $rank(X)=N$
\end_inset

.
 Let 
\begin_inset Formula $\omega:\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}\rightarrow\mathbb{R}$
\end_inset

 be a sharpness measure such that:
\begin_inset Formula 
\[
\forall w,u\in\prod_{i=1}^{m}\mathbb{R}^{d_{i}\times d_{i-1}}.\left(\exists\varepsilon>0.\ \phi\left(B\left(w,\varepsilon\right)\right)=\phi\left(B\left(u,\varepsilon\right)\right)\right)\rightarrow\omega(w)=\omega(u)
\]

\end_inset


\end_layout

\begin_layout Theorem
then any end-to-end solution is a flattest solution in the sense of 
\begin_inset Formula $\rho$
\end_inset

, namely: 
\begin_inset Formula $\Psi=\Psi_{0}$
\end_inset

.
\end_layout

\begin_layout Proof
By our assumption for any solution 
\begin_inset Formula $T\in\Psi$
\end_inset

 it holds that 
\begin_inset Formula $rank\left(T\right)\ge rank\left(TX\right)=rank\left(Y\right)=d_{y}$
\end_inset

.
 Specifically this holds for any flattest solution 
\begin_inset Formula $T_{0}\in\Psi_{0}$
\end_inset

, thus if 
\begin_inset Formula $\left(\kaliseries W,m\right)\in\varTheta_{0}\left(T_{0}\right)$
\end_inset

 is a flattest overparameterization, it holds that 
\begin_inset Formula $rank\left(W_{2:m}\right)\ge rank\left(T_{0}\right)=d_{y}$
\end_inset

.
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $T'\in\Psi$
\end_inset

 be a solution, by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "cor:dead-direction-local"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we know that there exists 
\begin_inset Formula $\left(\kaliseries{W'},m\right)\in\varTheta\left(T'\right)$
\end_inset

 and 
\begin_inset Formula $\varepsilon>0$
\end_inset

 such that:
\begin_inset Formula 
\[
\phi\left(B\left(\left(\kaliseries{\bar{W}},m\right),\varepsilon\right)\right)=\phi\left(B\left(\left(\kaliseries{W'},m\right),\varepsilon\right)\right)
\]

\end_inset


\end_layout

\begin_layout Proof
therefore by the locality of 
\begin_inset Formula $\omega$
\end_inset

:
\begin_inset Formula 
\[
\min_{T\in\Psi}\rho\left(T\right)=\rho\left(T_{0}\right)=\omega\left(\kaliseries{\bar{W}},m\right)=\omega\left(\kaliseries{W'},m\right)\ge\rho\left(T'\right)
\]

\end_inset


\end_layout

\begin_layout Proof
thus we conclude that 
\begin_inset Formula $T'\in\Psi_{0}.$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "main"
options "myStyle"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Deferred Proofs
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:proof-flat-unbalanced"

\end_inset

Proof of Proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "prop:FlatUnbalanced"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Proof
First, notice that the set 
\begin_inset Formula $\Omega_{0}\cap\mathcal{B}$
\end_inset

 is non-empty.
 Mulayoff et al.
 (2020) proposed the canonical flattest solution:
\begin_inset Formula 
\begin{equation}
W_{m}^{\ast}=US_{m}^{\frac{1}{m}},\quad W_{j}^{\ast}=S_{j}^{\frac{1}{m}},\quad W_{1}^{\ast}=S_{1}^{\frac{1}{m}}V^{T}\label{eq:Cannonical-sol}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
Where 
\begin_inset Formula $S_{j}$
\end_inset

 is used to denote the 
\begin_inset Formula $d_{j}\times d_{j-1}$
\end_inset

 matrix whose 
\begin_inset Formula $k$
\end_inset

th diagonal entry is 
\begin_inset Formula $\sigma_{k}\left(T\right)$
\end_inset

 and 
\begin_inset Formula $\sigma_{k}\left(T\right)$
\end_inset

 is the 
\begin_inset Formula $k$
\end_inset

th largest singular value of 
\begin_inset Formula $T$
\end_inset

.
 By 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sufficient"
plural "false"
caps "false"
noprefix "false"

\end_inset

 this is indeed a flattest solution, and one can readily see that it is
 also balanced.
\end_layout

\begin_layout Proof
Now let 
\begin_inset Formula $\left(\kaliseries W,m\right)\in\Omega_{0}\cap\mathcal{B}$
\end_inset

 be a flattest balanced solution which satisfies the sufficient condition
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sufficient"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we transform it into an unbalanced flattest solution.
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $k\in\left[m-1\right]$
\end_inset

 and denote the SVD decomposition of 
\begin_inset Formula $W_{k},\;W_{k+1}$
\end_inset

 by: 
\begin_inset Formula 
\[
W_{k}=U_{k}S_{k}V_{k}^{T},\;W_{k+1}=U_{k+1}S_{k+1}V_{k+1}^{T}
\]

\end_inset


\end_layout

\begin_layout Proof
Without loss of generality, assume that the elements along the main diagonal
 of 
\begin_inset Formula $S_{k}$
\end_inset

 are decreasing.
\end_layout

\begin_layout Proof
Because 
\begin_inset Formula $w$
\end_inset

 is balanced we know that: 
\begin_inset Formula $V_{k+1}S_{k+1}^{2}V_{k+1}^{T}=U_{k}S_{k}^{2}U_{k}^{T}$
\end_inset

 
\end_layout

\begin_layout Proof
For a given 
\begin_inset Formula $k$
\end_inset

, the two sides of the above equation are both orthogonal eigenvalue decompositi
ons of the same matrix.
 Denote its eigenvalues by 
\begin_inset Formula $\lambda_{1}^{k},...,\lambda_{r_{k}}^{k}$
\end_inset

and the multiplicity of the 
\begin_inset Formula $i$
\end_inset

th eigenvalue by 
\begin_inset Formula $\mu_{i}^{k}$
\end_inset

, there exists orthogonal matrices 
\begin_inset Formula $O_{k,i}\in\mathbb{R}^{\mu_{i}\times\mu_{i}}$
\end_inset

 such that:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{equation}
U_{k}=V_{k+1}\cdot diag\left(O_{k,1},...,O_{k,r_{k}}\right),\quad S_{k}=S_{k+1}\label{eq:SvdChain}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula $O_{k,i}$
\end_inset

 here is simply a matrix changing between orthonormal bases of the eigenspace
 of 
\begin_inset Formula $\lambda_{i}^{k}$
\end_inset

.
\end_layout

\begin_layout Proof
We create a flattest unbalanced solution by 
\begin_inset Quotes eld
\end_inset

perturbing
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $w$
\end_inset

:
\end_layout

\begin_layout Proof
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_deeper
\begin_layout Labeling
\labelwidthstring 00.00.0000

\bar under
If
\begin_inset space \space{}
\end_inset


\begin_inset Formula $\exists l\in[r].0<\sigma_{l}^{(k)}<\left(\sigma_{\max}\left(T\right)\right)^{\frac{1}{m}}$
\end_inset

:
\bar default
 Let 
\begin_inset Formula $\varepsilon>0$
\end_inset

 such that 
\begin_inset Formula $\left(1+\epsilon\right)\sigma_{l}^{(k)}\le\left(\sigma_{\max}\left(T\right)\right)^{\frac{1}{m}}$
\end_inset

, define 
\begin_inset Formula $\bar{S}_{k+1},\bar{S}_{k}$
\end_inset

 by:
\begin_inset Formula 
\[
\left(\bar{S}_{k}\right)_{i,j}=\begin{cases}
\left(S_{k}\right)_{i,j} & (i,j)\neq(l,l)\\
\frac{\sigma_{l}^{(k)}}{1+\varepsilon} & (i,j)=(l,l)
\end{cases}\;,\left(\bar{S}_{k+1}\right)=\begin{cases}
\left(S_{k+1}\right)_{i,j} & (i,j)\neq(l,l)\\
\left(1+\varepsilon\right)\sigma_{2}^{(k)} & (i,j)=(l,l)
\end{cases}
\]

\end_inset


\begin_inset Formula 
\[
\bar{W}_{k}=U_{k}\bar{S}_{k}V_{k}^{T},\;\bar{W}_{k+1}=U_{k+1}\bar{S}_{k+1}\left(V_{k+1}\cdot diag\left(O_{k,1},...,O_{k,r_{k}}\right)\right)^{T}
\]

\end_inset

 We define 
\begin_inset Formula $\bar{W}_{j}=W_{j}$
\end_inset

 for 
\begin_inset Formula $j\in\left[m\right]\backslash\left\{ k,k+1\right\} $
\end_inset

 and 
\begin_inset Formula $\bar{w}=vec\left(\kaliseries{\bar{W}},m\right)$
\end_inset

, by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:SvdChain"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we have:
\begin_inset Formula 
\begin{align*}
\bar{W}_{1:m} & =\prod_{j=k+2}^{m}W_{j}U_{k+1}\bar{S}_{k+1}{\color{gray}\underset{=I_{d_{k}}}{\underbrace{{\normalcolor \left(V_{k}\cdot diag\left(O_{k,1},...,O_{k,r_{k}}\right)\right)^{T}U_{k}}}}}\bar{S}_{k}V_{k}^{T}\prod_{j=1}^{k-1}W_{j}\\
 & =\prod_{j=k+2}^{m}W_{j}{\color{gray}\underset{\overset{\eqref{eq:SvdChain}}{=}W_{k+1}W_{k}}{\underbrace{{\normalcolor U_{k+1}\left(S_{k}^{2}\right)V_{k}^{T}}}}}W_{k+1}\prod_{j=1}^{k-1}W_{j}\\
 & =W_{1:m}=T\\
 & \Rightarrow\bar{W}_{1:m}\in\Omega
\end{align*}

\end_inset

 It remains to notice that by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sufficient"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\begin_inset Formula $\bar{w}\in\Omega_{0}$
\end_inset

 and by the contra-positive of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:SvdChain"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\begin_inset Formula $\bar{w}$
\end_inset

 is not balanced.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000

\bar under
If
\begin_inset space \space{}
\end_inset


\begin_inset Formula $\sigma_{2}^{(k)}=\sigma_{3}^{(k)}=\dotsc=\sigma_{r}^{(k)}=0$
\end_inset

:
\bar default
 We define a flattest unbalanced solution in a similar fashion:
\begin_inset Formula 
\[
\left(\bar{S}_{k+1}\right)=\begin{cases}
\left(S_{k+1}\right)_{i,j} & (i,j)\neq(2,2)\\
\frac{\left(\sigma_{\max}\left(T\right)\right)^{\frac{1}{m}}}{2} & (i,j)=(2,2)
\end{cases}\;,\quad\bar{W}_{k+1}=U_{k+1}\bar{S}_{k+1}\left(V_{k+1}\cdot diag\left(O_{k,1},...,O_{k,r_{k}}\right)\right)
\]

\end_inset

The results follows using the same arguments as for the case of 
\begin_inset Formula $\sigma_{2}^{(k)}>0$
\end_inset

.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000

\bar under
If
\begin_inset space \space{}
\end_inset


\begin_inset Formula $\forall\left(\kaliseries W,m\right)\in\left(\Omega_{0}\cap\mathcal{B}\right).$
\end_inset


\begin_inset Formula $\forall k\in[m].\sigma_{1}^{(k)}\ge\sigma_{2}^{(k)}\ge\dotsc\ge\sigma_{r}^{(k)}\ge\left(\sigma_{\max}\left(T\right)\right)^{\frac{1}{m}}:$
\end_inset


\bar default
 
\end_layout

\begin_deeper
\begin_layout Labeling
\labelwidthstring 00.00.0000
If
\begin_inset space \space{}
\end_inset


\begin_inset Formula $\exists\left(\kaliseries W,m\right)\in\left(\Omega_{0}\cap\mathcal{B}\right).$
\end_inset


\begin_inset Formula $\exists k\in[m].\sigma_{1}^{(k)}>\left(\sigma_{\max}\left(T\right)\right)^{\frac{1}{m}}$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Labeling
\labelwidthstring 00.00.0000
If
\begin_inset space \space{}
\end_inset


\begin_inset Formula $\forall k\in[m],l\in[rank\left(W_{k}\right)].\ \sigma_{l}^{(k)}>\left(\sigma_{k}\left(T\right)\right)^{\frac{1}{m}}\rightarrow u_{l}^{(k)}\in Ker\left(W_{k+1:m}\right)$
\end_inset

: We can decrease all such singular values to 
\begin_inset Formula $\left(\sigma_{k}\left(T\right)\right)^{\frac{1}{m}}$
\end_inset

 (and even 0), without changing the end-to-end solution: 
\begin_inset Formula $W_{1:m}$
\end_inset

.
 Denote the perturbed solution by 
\begin_inset Formula $\left(\kaliseries\bar{W},m\right)$
\end_inset

 we show by induction on 
\begin_inset Formula $k\in[m]$
\end_inset

 that 
\begin_inset Formula $\bar{W}_{1:m}x=\bar{W}_{k+1:m}W_{1:k}x$
\end_inset

 for 
\begin_inset Formula $x\in Im(X).$
\end_inset


\end_layout

\begin_deeper
\begin_layout Labeling
\labelwidthstring 00.00.0000
Induction
\begin_inset space \space{}
\end_inset

Base
\begin_inset space \space{}
\end_inset

(k=1): W.L.O.G we assume that only 
\begin_inset Formula $\sigma_{l}^{(1)}>\left(\sigma_{\max}\left(T\right)\right)^{\frac{1}{m}}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{1}
\bar{W}_{1:m}x & =\bar{W}_{2:m}\left(\sigma_{l}^{(1)}\left\langle x,v_{l}^{(1)}\right\rangle u_{l}^{(1)}+\sum_{l\neq i\in[d_{x}]}\sigma_{i}^{(1)}\left\langle x,v_{i}^{(1)}\right\rangle u_{i}^{(1)}\right)\\
 & \stackrel{}{=}\bar{W}_{2:m}\left(\sum_{l\neq i\in[d_{x}]}\sigma_{i}^{(1)}\left\langle x,v_{i}^{(1)}\right\rangle u_{i}^{(1)}\right)\\
 & =\bar{W}_{2:m}\left(\sum_{l\neq i\in[d_{x}]}\sigma_{i}^{(1)}\left\langle x,v_{i}^{(1)}\right\rangle u_{i}^{(1)}\right)=\bar{W}_{2:m}W_{1}x
\end{alignat*}

\end_inset


\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
Induction
\begin_inset space \space{}
\end_inset

step: Assume that the claim holds for all 
\begin_inset Formula $a\in[k]$
\end_inset

, again w.l.o.g assume that only 
\begin_inset Formula $\sigma_{l}^{(k+1)}>\left(\sigma_{\max}\left(T\right)\right)^{\frac{1}{m}}$
\end_inset

.
\begin_inset Formula 
\begin{alignat*}{1}
\bar{W}_{1:m}x & =\bar{W}_{k+1:m}\left(\sigma_{i}^{(k)}\left\langle \bar{W}_{1:k-1}x,v_{l}^{(k)}\right\rangle u_{l}^{(k)}+\sum_{l\neq i\in[d_{x}]}\sigma_{i}^{(k)}\left\langle \bar{W}_{1:k-1}x,v_{i}^{(k)}\right\rangle u_{i}^{(k)}\right)\\
 & \stackrel{i.h.}{=}\bar{W}_{k+1:m}\left(\sum_{l\neq i\in[d_{x}]}\sigma_{i}^{(k)}\left\langle W_{1:k-1}x,v_{i}^{(k)}\right\rangle u_{i}^{(k)}\right)=\bar{W}_{k+2:m}W_{1:k+1}x
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
Thus 
\begin_inset Formula $\left(\kaliseries\bar{W},m\right)\in\left(\Omega\cap\mathcal{B}^{C}\right),$
\end_inset

and by 
\begin_inset CommandInset ref
LatexCommand formatted
reference "thm:sufficient"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\begin_inset Formula $\left(\kaliseries\bar{W},m\right)\in\Omega_{0}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
Assume by contradiction that 
\begin_inset Formula $\exists k\in[m],l\in[r_{k}].\ \sigma_{l}^{(k)}>\left(\sigma_{k}\left(T\right)\right)^{\frac{1}{m}}\wedge u_{l}^{(k)}\notin Ker\left(W_{k+1:m}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $u_{l}^{(k)}\notin W_{1:k-1}X$
\end_inset

 we can set 
\begin_inset Formula $\sigma_{l}^{(k)}=\left(\sigma_{k}\left(T\right)\right)^{\frac{1}{m}}$
\end_inset

 and using a similar induction we show that the perturbed end-to-end matrix
 is a solution.
\end_layout

\begin_layout Standard
Otherwise, we know that 
\begin_inset Formula $\exists x\in Im\left(X\right).W_{1:k-1}x=u_{l}^{(k)}$
\end_inset

.
 Notice that: 
\begin_inset Formula 
\[
\sigma_{\min}\left(W_{k+1:m}\right)\ge\prod_{j=k+1}^{m}\sigma_{\min}^{(j)}\ge\left(\sigma_{\max}\left(T\right)\right)^{\frac{m-k+1}{m}},\quad\sigma_{\min}\left(W_{1:k-1}\right)\ge\prod_{j=1}^{k-1}\sigma_{\min}^{(j)}\ge\left(\sigma_{\max}\left(T\right)\right)^{\frac{k-1}{m}}
\]

\end_inset


\begin_inset Formula 
\begin{align*}
\Rightarrow\sigma_{\max}\left(T\right)\left\Vert x\right\Vert _{2} & \ge\left\Vert W_{1:m}x\right\Vert \\
 & \ge\left(\sigma_{\max}\left(T\right)\right)^{\frac{k-1}{m}}\sigma_{l}^{(k)}\left\Vert W_{k+1:m}u_{l}^{(k)}\right\Vert _{2}\left\Vert x\right\Vert _{2}\\
 & \ge\left(\sigma_{\max}\left(T\right)\right)^{\frac{m-1}{m}}\sigma_{l}^{(k)}\left\Vert x\right\Vert _{2}\\
 & >\sigma_{\max}\left(T\right)\left\Vert x\right\Vert _{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where the first inequality is true because the 
\begin_inset Formula $l_{2}$
\end_inset

 operator norm is induced.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "subsec:proof-of-hes-trace"

\end_inset

Proof of Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:hes-trace"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Proof
Following Mulayoff et.al.
 (2020), we know that the hessian at a global minimum has the following
 form:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
H_{w}=2\Phi\Phi^{T}
\]

\end_inset


\begin_inset Formula 
\[
\Phi=\left[\Phi_{1}^{T},\Phi_{2}^{T},...,\Phi_{m}^{T}\right]{}^{T}
\]

\end_inset


\begin_inset Formula 
\[
\Phi_{k}=\left(\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}\right)\odot\left(\prod_{i=k+1}^{m}W_{i}\right)^{T}
\]

\end_inset


\end_layout

\begin_layout Proof
Where 
\begin_inset Formula $\odot$
\end_inset

 is the kronecker product.
\end_layout

\begin_layout Proof
Thus we get the follwing:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
Sharp(w)=Trace(H_{w})=Trace(2\Phi\Phi^{T})=2\sum_{i=1}^{m}Trace(\Phi_{i}\Phi_{i}^{T})
\]

\end_inset


\begin_inset Formula 
\begin{alignat}{1}
Trace(\Phi_{i}\Phi_{i}^{T}) & =Trace\left(\left[\left(\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}\right)\odot\left(\prod_{i=k+1}^{m}W_{i}\right)^{T}\right]\left[\left(\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}\right)\odot\left(\prod_{i=k+1}^{m}W_{i}\right)^{T}\right]^{T}\right)\nonumber \\
 & =Trace\left(\left[\left(\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}\right)\odot\left(\prod_{i=k+1}^{m}W_{i}\right)^{T}\right]\left[\left(\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}\right)^{T}\odot\left(\prod_{i=k+1}^{m}W_{i}\right)\right]\right)\label{eq:trace}\\
 & =Trace\left(\left[\left(\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}\right)\left(\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}\right)^{T}\right]\odot\left[\left(\prod_{i=k+1}^{m}W_{i}\right)^{T}\left(\prod_{i=k+1}^{m}W_{i}\right)\right]\right)\nonumber \\
 & =Trace\left(\left(\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}\right)\left(\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}\right)^{T}\right)\cdot Trace\left(\left(\prod_{i=k+1}^{m}W_{i}\right)^{T}\left(\prod_{i=k+1}^{m}W_{i}\right)\right)\nonumber \\
 & =||\prod_{j=1}^{k-1}W_{j}\Sigma_{x}^{0.5}||_{F}^{2}\cdot||\prod_{i=k+1}^{m}W_{i}||_{F}^{2}\nonumber 
\end{alignat}

\end_inset

 
\end_layout

\begin_layout Proof
By our assumption, 
\begin_inset Formula $\Sigma_{x}^{-1}=Id$
\end_inset

, turning 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

 into:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{equation}
Trace(\Phi_{i}\Phi_{i}^{T})=||\prod_{j=1}^{k-1}W_{j}||_{F}^{2}\cdot||\prod_{i=k+1}^{m}W_{i}||_{F}^{2}=||W_{k-1:1}||_{F}^{2}\cdot||W_{m:k+1}||_{F}^{2}\label{eq:trace2}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
Where we used the following notation:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
W_{n:k} & :=\prod_{i=k}^{n}W_{i}=W_{n}W_{n-1}...W_{k}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Using 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:trace2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we get the following:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
Trace(H_{w})=2\sum_{k=1}^{m}||W_{k-1:1}||_{F}^{2}\cdot||W_{m:k+1}||_{F}^{2}
\]

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:proof-of-non-square-case"

\end_inset

Proof of Lemma 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:non-square-case"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $T=USV^{T}\in\mathbb{R}^{d_{X}\times d_{Y}}$
\end_inset

 be the SVD decomposition, where: 
\begin_inset Formula 
\[
S=\left[\begin{array}{ccccc}
x & 0 & \cdots & \cdots & 0\\
0 & 1 & \ddots &  & \vdots\\
\vdots & \ddots & 0 & \ddots & \vdots\\
\vdots &  & \ddots & \ddots & 0\\
0 & \cdots & \cdots & 0 & 0
\end{array}\right]\in\mathbb{R}^{d_{X}\times d_{Y}}
\]

\end_inset


\end_layout

\begin_layout Proof
Assume that we have a solution: 
\begin_inset Formula $T=W_{2}W_{1}$
\end_inset

.
 In the non-square case it holds that:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align}
Trace(H_{w}) & =2\sum_{k=1}^{m}||W_{k-1:1}||_{F}^{2}\cdot||W_{m:k+1}||_{F}^{2}=\nonumber \\
 & =2\left(||W_{0:1}||_{F}^{2}\cdot||W_{2:2}||_{F}^{2}+||W_{1:1}||_{F}^{2}\cdot||W_{2:3}||_{F}^{2}\right)\nonumber \\
 & =2\left(||I_{d_{Y}}||_{F}^{2}\cdot||W_{2}||_{F}^{2}+||W_{1}||_{F}^{2}\cdot||I_{d_{X}}||_{F}^{2}\right)\label{eq:non_square_trace}\\
 & =2\left(d_{y}||W_{2}||_{F}^{2}+d_{X}||W_{1}||_{F}^{2}\right)\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $W_{2}=US^{\frac{2}{3}}$
\end_inset

 and 
\begin_inset Formula $W_{1}=S^{\frac{1}{3}}V^{T}$
\end_inset

.
 It is easy to see that it is a non-balanced solution.
 Assume that 
\begin_inset Formula $d_{X}=\alpha\cdot d_{y}$
\end_inset

 for some 
\begin_inset Formula $\alpha>1$
\end_inset

.
\end_layout

\begin_layout Proof
By 
\begin_inset Formula $\text{\ref{eq:non_square_trace}}$
\end_inset

We get the following:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align}
Trace(H_{w}) & =2\left(d_{y}||W_{2}||_{F}^{2}+d_{X}||W_{1}||_{F}^{2}\right)\nonumber \\
 & =2\left(d_{y}||US^{\frac{2}{3}}||_{F}^{2}+\alpha\cdot d_{y}||S^{\frac{1}{3}}V^{T}||_{F}^{2}\right)\label{counter_trace}\\
 & =2\left(d_{y}\left(x^{\frac{4}{3}}+1\right)+\alpha\cdot d_{y}\left(x^{\frac{2}{3}}+1\right)\right)\nonumber \\
 & =2d_{y}\left(x^{\frac{4}{3}}+\alpha x^{\frac{2}{3}}+\alpha+1\right)\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Proof
And for the balanced solution we have the following:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
Trace(H_{w_{bal}}) & =2\sum_{k=1}^{m}||S^{\frac{k-1}{m}}||_{F}^{2}\cdot||S^{\frac{m-k}{m}}||_{F}^{2}\\
 & =2\left(||I_{d_{Y}}||_{F}^{2}\cdot||S^{\frac{1}{2}}||_{F}^{2}+||S^{\frac{1}{2}}||_{F}^{2}\cdot||I_{d_{X}}||_{F}^{2}\right)\\
 & =2\left(d_{y}\cdot\left(x+1\right)+\alpha\cdot d_{y}\left(x+1\right)\right)\\
 & =2d_{y}\left(x+\alpha x+1+\alpha\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
We want toe get a counter example by having the balanced trace be larger
 than the non-balanced trace, i.e:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{flalign*}
2d_{y}\left(x+\alpha x+1+\alpha\right) & >2d_{y}\left(x^{\frac{4}{3}}+\alpha x^{\frac{2}{3}}+\alpha+1\right)\\
x+\alpha x & >x^{\frac{4}{3}}+\alpha x^{\frac{2}{3}}\\
x^{\frac{4}{3}}-x-\alpha x+\alpha x^{\frac{2}{3}} & <0
\end{flalign*}

\end_inset


\end_layout

\begin_layout Proof
Solving the inequality results in the following:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
x\in\left(1,\alpha^{3}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
Thus for 
\begin_inset Formula $1<x<\alpha^{3}$
\end_inset

 we get a counter example, proving that balanced solution is not the flattest,
 and that a flattest solution is not balanced, as all ballanced solution
 have the same flattness.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:proof-of-2-layer-case"

\end_inset

Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "thm:2-layer-case"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $T=W_{2}W_{1}$
\end_inset

 such that 
\begin_inset Formula $T\in\mathbb{R}^{d\times d}$
\end_inset

 and 
\begin_inset Formula $W_{2},W_{1}^{T}\in\mathbb{R}^{d\times d_{h}}$
\end_inset

.
 Define 
\begin_inset Formula $w=\left(W_{2,}W_{1}\right)$
\end_inset


\end_layout

\begin_layout Proof
By 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hes-trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

we get: 
\begin_inset Formula 
\[
Trace(H_{w})=2\left(||W_{0:1}||_{F}^{2}\cdot||W_{2:2}||_{F}^{2}+||W_{1:1}||_{F}^{2}\cdot||W_{2:3}||_{F}^{2}\right)=2d\left(||W_{1}||_{F}^{2}+||W_{2}||_{F}^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
Now we want to minimize the trace.
 Let 
\begin_inset Formula $T=USV^{T}$
\end_inset

 be the SVD decompostion of T.
 Then it holds that:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{equation}
\underset{W_{2}W_{1}=T}{\min}\left(||W_{1}||_{F}^{2}+||W_{2}||_{F}^{2}\right)=2||S^{\frac{1}{2}}||_{F}^{2}=2||S||_{\sigma}\label{eq:flattest-2-layer}
\end{equation}

\end_inset

where 
\begin_inset Formula $||\cdot||_{\sigma}$
\end_inset

 is the nuclear norm.
 To see that we use AM-GM and get the following:
\begin_inset Formula 
\[
||W_{1}||_{F}^{2}+||W_{2}||_{F}^{2}\geq2\sqrt{||W_{1}||_{F}^{2}\cdot||W_{2}||_{F}^{2}}=2||W_{1}||_{F}||W_{2}||_{F}
\]

\end_inset


\end_layout

\begin_layout Proof
Using Cauchy-Schwarz inequality we get:
\begin_inset Formula 
\begin{alignat*}{1}
||W_{1}||_{F}||W_{2}||_{F} & =||U^{T}W_{2}||_{F}\cdot||VW_{1}^{T}||_{F}\\
 & \geq\langle U^{T}W_{2},VW_{1}^{T}\rangle_{F}\\
 & =Trace\left(U^{T}W_{2}W_{1}V\right)\\
 & =Trace\left(U^{T}USV^{T}V\right)\\
 & =Trace\left(S\right)\\
 & =||S^{\frac{1}{2}}||_{F}^{2}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Thus:
\begin_inset Formula 
\[
||W_{1}||_{F}^{2}+||W_{2}||_{F}^{2}\geq2||S^{\frac{1}{2}}||_{F}^{2}
\]

\end_inset


\end_layout

\begin_layout Proof
With equality for 
\begin_inset Formula $W_{2}=US^{\frac{1}{2}},W_{1}=S^{\frac{1}{2}}V^{T}$
\end_inset

.
\end_layout

\begin_layout Proof
Using equation 
\begin_inset Formula $\left(\text{\ref{eq:balanced_trace}}\right)$
\end_inset

 in the 2 layer case we get:
\begin_inset Formula 
\[
Trace(H_{w})=2\sum_{k=1}^{2}||S^{\frac{k-1}{2}}||_{F}^{2}\cdot||S^{\frac{2-k}{2}}||_{F}^{2}=2\left(||S^{0}||_{F}^{2}\cdot||S^{\frac{1}{2}}||_{F}^{2}+||S^{\frac{1}{2}}||_{F}^{2}\cdot||S^{0}||_{F}^{2}\right)=4d||S^{\frac{1}{2}}||_{F}^{2}
\]

\end_inset


\end_layout

\begin_layout Proof
Which is indeed the flattest solution by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:flattest-2-layer"
plural "false"
caps "false"
noprefix "false"

\end_inset

, completing the first direction of the theorem.
\end_layout

\begin_layout Proof
To show the other direction, assume that 
\begin_inset Formula $W_{1},W_{2}$
\end_inset

 is a flattest solution such that: 
\begin_inset Formula $W_{2},W_{1}^{T}\in\mathbb{R}^{d\times d_{h}}$
\end_inset

.
 Thus 
\begin_inset Formula $W_{1,}W_{2}$
\end_inset

 satisfy: 
\begin_inset Formula 
\[
W_{1},W_{2}=\underset{W_{2}W_{1}=T}{\argmin}\left(||W_{1}||_{F}^{2}+||W_{2}||_{F}^{2}\right)=\frac{1}{2d}Trace(H_{w})
\]

\end_inset


\end_layout

\begin_layout Proof
We define the following function:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
f(X)=||W_{2}X||_{F}^{2}+||X^{-1}W_{1}||_{F}^{2}
\]

\end_inset


\end_layout

\begin_layout Proof
The function 
\begin_inset Formula $f$
\end_inset

 is defined on the open set of invertible 
\begin_inset Formula $d_{h}\times d_{h}$
\end_inset

 matrices, thus any local minima satisfies:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\nabla f(X_{min})=0
\]

\end_inset


\end_layout

\begin_layout Proof
In particular, 
\begin_inset Formula $X=Id$
\end_inset

 is a local minima by defenition of 
\begin_inset Formula $W_{1},W_{2}$
\end_inset

.
\end_layout

\begin_layout Proof
Next we want to calculate the gradient of 
\begin_inset Formula $f$
\end_inset

.
\end_layout

\begin_layout Proof
First let us look into:
\begin_inset Formula 
\[
g_{A}(X)=||XA||_{F}^{2}=\sum_{i}\sum_{j}\left(\sum_{k}X_{i,k}A_{k,j}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\frac{\partial}{\partial X_{s,t}}g(X)=\frac{\partial}{\partial X_{s,t}}\sum_{i}\sum_{j}\left(\sum_{k}X_{i,k}A_{k,j}\right)^{2}=\frac{\partial}{\partial X_{s,t}}\sum_{j}\left(\sum_{k}X_{s,k}A_{k,j}\right)^{2}=2\sum_{j}\left(\sum_{k}X_{s,k}A_{k,j}\right)A_{t,j}=2\left(XAA^{T}\right)_{s,t}
\]

\end_inset


\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\nabla g\left(X\right)=2XAA^{T}
\]

\end_inset


\end_layout

\begin_layout Proof
Using the same calculation, we get that:
\begin_inset Formula 
\[
\frac{\partial}{\partial X}||X^{-1}A||_{F}^{2}=X^{-T}X^{-1}AA^{T}X^{-T}
\]

\end_inset


\end_layout

\begin_layout Proof
Plugging 
\begin_inset Formula $X=Id$
\end_inset

 we get:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
0=\nabla f\left(Id\right)=W_{2}^{T}W_{2}-W_{1}W_{1}^{T}\Rightarrow W_{1}W_{1}^{T}=W_{2}^{T}W_{2}
\]

\end_inset


\end_layout

\begin_layout Proof
Which is the condition of exact balancedness, showing that flattest solution
 is indeed balanced and completing the proof.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:proof-of-m-layer-case"

\end_inset

Proof for Lemma 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:m-layer-case"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Proof
We show a counter example here, by providing a solution wich has lower sharpness
 than any balanced solutiom.
\end_layout

\begin_layout Proof
Let:
\begin_inset Formula 
\[
S=diag\left(1,x,0,...,0\right)\in\mathbb{R}^{d\times d},x\in\left(0,1\right)
\]

\end_inset


\end_layout

\begin_layout Proof
Such that 
\begin_inset Formula $T=USV^{T}$
\end_inset

 is the SVD decompostion of the end-to-end solution.
\end_layout

\begin_layout Proof
For a balanced solution 
\begin_inset Formula $T=W_{m}\cdot...\cdot W_{1},w=(W_{m},...,W_{1})$
\end_inset

 we know from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:balanced_trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

that it holds:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
Trace(H_{w})=2\sum_{k=1}^{m}||S^{\frac{k-1}{m}}||_{F}^{2}\cdot||S^{\frac{m-k}{m}}||_{F}^{2}
\]

\end_inset


\end_layout

\begin_layout Proof
Plugging the value of 
\begin_inset Formula $S$
\end_inset

 we get:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
Trace(H_{w}) & =2\left(2dx^{\frac{2}{m}(m-1)}+\sum_{k=2}^{m-1}\left(x^{\frac{2}{m}(k-1)}+1\right)\left(x^{\frac{2}{m}(m-k)}+1\right)\right)\\
 & \stackrel{t=x^{\frac{2}{m}}}{=}2\left(2dt^{m-1}+\left(m-2\right)t^{m-1}+m-2+2\sum_{k=1}^{m-2}t^{k}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Now, we define another solution:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\hat{W}_{m}=US^{\frac{1}{2}},\hat{W}_{1}=S^{\frac{1}{2}}V^{T},\hat{W}_{i}=I_{n},\hat{w}=\left(\hat{W}_{m},...,\hat{W}_{1}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
For this solution we know it holds from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:hes-trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
Trace(H_{\hat{w}}) & =2\sum_{k=1}^{m}||W_{k-1:1}||_{F}^{2}\cdot||W_{m:k+1}||_{F}^{2}\\
 & =2\left(2d\left(x+1\right)+\sum_{k=2}^{m-1}\left(x+1\right)\left(x+1\right)\right)\\
 & \stackrel{t=x^{\frac{2}{m}}}{=}2\left(\left(m-2\right)t^{m}+2\left(m-2+d\right)t^{\frac{m}{2}}\right)
\end{align*}

\end_inset

 Now we define the following function:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
f(t)=\frac{Trace(H_{\hat{w}})-Trace(H_{w})}{2}=\left(m-2\right)t^{m}+2\left(m-2+d\right)t^{\frac{m}{2}}-\left(2dt^{m-1}+\left(m-2\right)t^{m-1}+m-2+2\sum_{k=1}^{m-2}t^{k}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
Notice that 
\begin_inset Formula $f(0)=0$
\end_inset

.
 Now we look at 
\begin_inset Formula $f'(0)$
\end_inset

:
\begin_inset Formula 
\begin{align*}
f'(t) & =m\left(m-2\right)t^{m-1}+m\left(m-2+d\right)t^{\frac{m}{2}-1}-\left(2d\left(m-1\right)t^{m-2}+\left(m-2\right)\left(m-1\right)t^{m-2}+2\sum_{k=1}^{m-2}kt^{k-1}\right)\\
f'(0) & =-2
\end{align*}

\end_inset

 We got that the derivative at 0 is negative, thus exists 
\begin_inset Formula $t>0$
\end_inset

 small enough such that 
\begin_inset Formula $f(t)<0$
\end_inset

, which means 
\begin_inset Formula $Trace(H_{w})>Trace(H_{\hat{w}})$
\end_inset

.
\end_layout

\begin_layout Proof
By 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:balanced_trace"
plural "false"
caps "false"
noprefix "false"

\end_inset

, all balnced solution share the same sharpness, thus any balanced solution
 is not the flattest and any flattest solution is not balanced.
\end_layout

\begin_layout Subsection
Proof of Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lemma:unif-lb"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Proof
For the first part, 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Mulayoff2020Unique"
literal "false"

\end_inset

 have showed that 
\begin_inset Formula $\max_{\left\Vert B\right\Vert _{F}=1}2m\left\Vert \left(B\hat{\Sigma}_{x}^{\frac{1}{2}}T^{T}\right)^{m-1}B\hat{\Sigma}_{x}^{\frac{1}{2}}\right\Vert _{2}^{\frac{2}{m}}$
\end_inset

 lower bounds the flatness of the single solution 
\begin_inset Formula $T$
\end_inset

 in the non-singular case.
 Note that this bound is independent of the specific overparameterization
 of T .Their analysis carries through to the singular case, where now apriori
 the bound is dependent on the end-to-end solution.
\end_layout

\begin_layout Proof
In this case, let 
\begin_inset Formula $T\in\Psi$
\end_inset

 and denote 
\begin_inset Formula $T=YX^{+}+K\left(I_{d_{x}}-XX^{+}\right)$
\end_inset

, plugging this into the above expression yields:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
\max_{\left\Vert B\right\Vert _{F}=1}\nu_{T}\left(B\right) & =2m\left\Vert \left(B\hat{\Sigma}_{x}^{\frac{1}{2}}T^{T}\right)^{m-1}B\hat{\Sigma}_{x}^{\frac{1}{2}}\right\Vert _{2}^{\frac{2}{m}}\\
 & \stackrel{\ref{assumption:pseudo-white}}{=}\max_{\left\Vert B\right\Vert _{F}=1}2m\left\Vert \left(BXX^{T}\left(YX^{+}+K\left(I_{d_{x}}-XX^{+}\right)\right)^{T}\right)^{m-1}BXX^{T}\right\Vert _{2}^{\frac{2}{m}}\\
 & =\max_{\left\Vert B\right\Vert _{F}=1}2m\left\Vert \left(BXX^{T}\left(\left(X^{T}\right)^{+}Y^{T}+\left(I_{d_{x}}-\left(X^{T}\right)^{+}X^{T}\right)K^{T}\right)\right)^{m-1}BXX^{T}\right\Vert _{2}^{\frac{2}{m}}\\
 & =\max_{\left\Vert B\right\Vert _{F}=1}2m\left\Vert \left(BXY^{T}\right)^{m-1}BXX^{T}\right\Vert _{2}^{\frac{2}{m}}\\
 & \le\max_{\left\Vert B\right\Vert _{F}=1}2m\left(\left\Vert B\right\Vert _{2}\left\Vert XY^{T}\right\Vert _{2}\right)^{2\left(1-\frac{1}{m}\right)}\left\Vert B\right\Vert _{2}^{\frac{2}{m}}\left\Vert XX^{T}\right\Vert _{2}^{\frac{2}{m}}\\
 & \le2m\left\Vert XY^{T}\right\Vert ^{2\left(1-\frac{1}{m}\right)}\left\Vert XX^{T}\right\Vert _{2}^{\frac{2}{m}}\\
 & =2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Where the equality in the third and fourth line are due to to the properties
 of the Moore-Penrose pseudo-inverse and 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The inequality in the fith line used norm sub-multiplicity, that of the
 sixth is due to the fact that 
\begin_inset Formula $\left\Vert B\right\Vert _{2}\le\left\Vert B\right\Vert _{F}$
\end_inset

, and the seventh by 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and the fact that 
\begin_inset Formula $\sigma_{\max}\left(YX^{T}\right)=\text{\sigma_{\max}\left(X^{T}Y\right)}$
\end_inset

.
\end_layout

\begin_layout Proof
We now show that this bound is achieved with equality: 
\end_layout

\begin_layout Proof
Denote the SVD of 
\begin_inset Formula $YX^{T}$
\end_inset

by 
\begin_inset Formula $USV^{T}$
\end_inset

, where 
\begin_inset Formula $\left(S\right)_{i,i}\ge\left(S\right)_{i+1,i+1},\quad i\in\left[d_{y}-1\right]$
\end_inset

, and the left, right singular vectors corresponding to the maximal singular
 value by 
\begin_inset Formula $u,$
\end_inset

v respectively.
\end_layout

\begin_layout Proof
Taking 
\begin_inset Formula $B=uv^{T}$
\end_inset

gives the following bound:
\begin_inset Formula 
\begin{alignat*}{2}
\nu_{T}\left(uv^{T}\right) & = & 2m\left\Vert \left(uv^{T}XX^{T}X\left(X^{T}X\right)^{-1}Y^{T}\right)^{m-1}uv^{T}XX^{T}\right\Vert _{2}^{\frac{2}{m}}\\
 & = & 2m\left\Vert u\left(v^{T}XY^{T}u\right)^{m-1}v^{T}XX^{T}\right\Vert _{2}^{\frac{2}{m}}\\
 & = & 2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}\left\Vert uv^{T}XX^{T}\right\Vert _{2}^{\frac{2}{m}}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
It remains to show that 
\begin_inset Formula $\left\Vert uv^{T}XX^{T}\right\Vert _{2}=1$
\end_inset

 .
 First notice that 
\begin_inset Formula $uv^{T}$
\end_inset

 has only the nonzero singular value 1 of multiplicity one, and that by
 
\begin_inset CommandInset ref
LatexCommand ref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\begin_inset Formula $\left\Vert XX^{T}\right\Vert _{2}=1$
\end_inset

, thus:
\begin_inset Formula 
\[
\left\Vert uv^{T}XX^{T}\right\Vert _{2}\le\left\Vert uv^{T}\right\Vert _{2}\left\Vert XX^{T}\right\Vert _{2}=1
\]

\end_inset


\end_layout

\begin_layout Proof
Next we show that this bound is achieved by 
\begin_inset Formula $v$
\end_inset

.
 To show this notice that by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "assumption:pseudo-white"
plural "false"
caps "false"
noprefix "false"

\end_inset

:
\begin_inset Formula 
\[
XX^{T}v=\begin{pmatrix}I_{n} & \mathbb{0}_{n\times\left(d_{x}-n\right)}\\
\mathbb{0}_{\left(d_{x}-n\right)\times n} & \mathbb{0}_{d_{x}-n}
\end{pmatrix}v=v\Leftrightarrow v\in\left(\mathbb{R}^{N}\times\{0\}^{d_{x}-N}\right)
\]

\end_inset

We claim that the right-hand side must hold, to see this let 
\begin_inset Formula $X=\sum_{i=1}^{N}\sigma_{i}\left(X\right)u_{X}^{i}\left(v_{X}^{i}\right)^{T}\text{,}\ Y=\sum_{i=1}^{r_{Y}}\sigma_{i}\left(Y\right)u_{Y}^{i}\left(v_{Y}^{i}\right)^{T}$
\end_inset

 be the SVD of 
\begin_inset Formula $X,Y$
\end_inset

 respectively.
 By definition it holds that:
\begin_inset Formula 
\begin{alignat*}{1}
\max_{\left\Vert w\right\Vert _{2}=1}\left\Vert YX^{T}w\right\Vert _{2}^{2} & =\left\Vert YX^{T}v\right\Vert _{2}^{2}\\
 & =\left\Vert Y\left(\sum_{j=1}^{N}\sigma_{j}\left(X\right)v_{X}^{j}\left(u_{X}^{j}\right)^{T}\right)\left(\sum_{i=1}^{d_{x}}\left\langle v,u_{X}^{i}\right\rangle u_{X}^{i}\right)\right\Vert _{2}^{2}\\
 & =\left\Vert Y\left(\sum_{j=1}^{N}\sum_{i=1}^{d_{x}}\sigma_{j}\left(X\right)\left\langle v,u_{X}^{i}\right\rangle v_{X}^{j}\left\langle u_{X}^{j},u_{X}^{i}\right\rangle \right)\right\Vert _{2}^{2}\\
 & =\left\Vert Y\left(\sum_{j=1}^{N}\sigma_{j}\left(X\right)\left\langle v,u_{X}^{j}\right\rangle v_{X}^{j}\right)\right\Vert _{2}^{2}\\
 & =\left\Vert \left(\sum_{i=1}^{r_{Y}}\sigma_{i}\left(Y\right)u_{Y}^{i}\left(v_{Y}^{i}\right)^{T}\right)\left(\sum_{j=1}^{N}\sigma_{j}\left(X\right)\left\langle v,u_{X}^{j}\right\rangle v_{X}^{j}\right)\right\Vert _{2}^{2}\\
 & =\left\Vert \sum_{i=1}^{r_{Y}}\left(\sum_{j=1}^{N}\sigma_{i}\left(Y\right)\sigma_{j}\left(X\right)\left\langle v,u_{X}^{j}\right\rangle \left\langle v_{Y}^{i},v_{X}^{j}\right\rangle \right)u_{Y}^{i}\right\Vert _{2}^{2}\\
 & =\sum_{i=1}^{r_{Y}}\left(\sum_{j=1}^{N}\sigma_{i}\left(Y\right)\sigma_{j}\left(X\right)\left\langle v_{Y}^{i},v_{X}^{j}\right\rangle \left\langle v,u_{X}^{j}\right\rangle \right)^{2}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
Where the last equality is due to Pythagoras theorem.
\end_layout

\begin_layout Proof
Assume by contradiction that 
\begin_inset Formula $v\notin\left(\mathbb{R}^{N}\times\{0\}^{d_{x}-N}\right)$
\end_inset

, i.e.
 
\begin_inset Formula $\exists k\in\left\{ N+1,N+2,...,d_{x}\right\} .(v)_{k}\neq0$
\end_inset

.
\end_layout

\begin_layout Proof
Note that:
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

label=
\backslash
alph{enumi}.
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Formula $0<\sum_{j=1}^{N}\left\langle v,u_{X}^{j}\right\rangle ^{2}$
\end_inset

: Otherwise 
\begin_inset Formula $v=\sum_{j=N+1}^{d_{x}}\left\langle v,u_{X}^{j}\right\rangle ^{2}u_{X}^{j}$
\end_inset

 yielding 
\begin_inset Formula $YX^{T}v=Y\mathbb{0}_{n}=\mathbb{0}_{d_{y}}$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\sum_{j=1}^{N}\left\langle v,u_{X}^{j}\right\rangle ^{2}<1$
\end_inset

: By the Pythagoras theorem and the fact that
\begin_inset Formula $\left\Vert v\right\Vert _{2}=1$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Proof
Define 
\begin_inset Formula $\bar{v}=\left(\sum_{k\neq j=1}^{d_{x}}\left\langle v,u_{X}^{j}\right\rangle ^{2}\right)^{-\frac{1}{2}}\sum_{k\neq j=1}^{d_{x}}\left\langle v,u_{X}^{j}\right\rangle u_{X}^{j}$
\end_inset

 , by construction
\begin_inset Formula $\left\Vert \bar{v}\right\Vert =1$
\end_inset

.
 Plugging it in the above derivation yields the following contradiction:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{alignat*}{1}
\left\Vert YX^{T}\bar{v}\right\Vert _{2}^{2} & =\sum_{i=1}^{r_{Y}}\left(\sum_{j=1}^{N}\sigma_{i}\left(Y\right)\sigma_{j}\left(X\right)\left\langle v_{Y}^{i},v_{X}^{j}\right\rangle \left\langle \bar{v},u_{X}^{j}\right\rangle \right)^{2}\\
 & =\left(\sum_{k\neq j=1}^{d_{x}}\left\langle v,u_{X}^{j}\right\rangle ^{2}\right)^{-1}\sum_{i=1}^{r_{Y}}\left(\sum_{j=1}^{N}\sigma_{i}\left(Y\right)\sigma_{j}\left(X\right)\left\langle v_{Y}^{i},v_{X}^{j}\right\rangle \left\langle v,u_{X}^{j}\right\rangle \right)^{2}\\
 & =\left(\sum_{k\neq j=1}^{d_{x}}\left\langle v,u_{X}^{j}\right\rangle ^{2}\right)^{-1}\left\Vert YX^{T}v\right\Vert _{2}^{2}\\
 & >\left\Vert YX^{T}v\right\Vert _{2}^{2}\\
 & =\max_{\left\Vert w\right\Vert _{2}=1}\left\Vert YX^{T}w\right\Vert _{2}^{2}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Proof
We have showed that 
\begin_inset Formula $v\in\left(\mathbb{R}^{N}\times\{0\}^{d_{x}-N}\right)$
\end_inset

 hence 
\begin_inset Formula $XX^{T}v=v$
\end_inset

.
 The lower bound now follows from the definition:
\begin_inset Formula 
\begin{alignat*}{2}
\left\Vert uv^{T}XX^{T}\right\Vert _{2} & = & \max_{\left\Vert w\right\Vert _{2}=1}\left\Vert uv^{T}XX^{T}w\right\Vert _{2}\\
 & \ge & \left\Vert uv^{T}XX^{T}v\right\Vert _{2}\\
 & = & \left\Vert u\right\Vert _{2}\\
 & = & 1
\end{alignat*}

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:proof-of-lem-acheived"

\end_inset

Proof of Lemma 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:lower-bound-achieved"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Proof
Note that under these assumptions 
\begin_inset Formula $YX^{T}\in\Psi$
\end_inset

.
 Let us denote its SVD by 
\begin_inset Formula $USV^{T},$
\end_inset

 we define the canonical solution as in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Cannonical-sol"
plural "false"
caps "false"
noprefix "false"

\end_inset

 by:
\begin_inset Formula 
\begin{equation}
W_{m}^{\ast}=US_{m}^{\frac{1}{m}},\quad W_{j}^{\ast}=S_{j}^{\frac{1}{m}}\quad j\in\left\{ 2,3,...,m-1\right\} ,\quad W_{1}^{\ast}=S_{1}^{\frac{1}{m}}V^{T}\label{eq:canon-singular}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
Following the proof given in 
\begin_inset CommandInset citation
LatexCommand citep
key "Mulayoff2020Unique"
literal "false"

\end_inset

 we get that:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
\lambda_{\mathrm{\max}}\left(H_{w^{\ast}}\right) & = & \max_{\left\Vert \tilde{B}\right\Vert _{F}=1}2\sum_{k=1}^{m}\left\Vert \left(S^{\frac{m-k}{m}}\right)^{T}\tilde{B}\hat{\Sigma}_{x}^{\frac{1}{2}}\left(S^{\frac{k-1}{m}}\right)^{T}\right\Vert _{F}^{2}\\
 & = & \max_{\left\Vert \tilde{B}\right\Vert _{F}=1}2\sum_{k=1}^{m}\left\Vert \left(S^{\frac{m-k}{m}}\right)^{T}\tilde{B}\begin{pmatrix}I_{N} & \mathbb{0}_{N\times\left(d_{x}-N\right)}\\
\mathbb{0}_{\left(d_{x}-N\right)\times N} & \mathbb{0}_{d_{x}-N}
\end{pmatrix}\left(S^{\frac{k-1}{m}}\right)^{T}\right\Vert _{F}^{2}\\
 & = & \max_{\left\Vert \tilde{B}\right\Vert _{F}=1}2\sum_{k=1}^{m}\sum_{i=1}^{N}\sum_{j=1}^{N}\left[\left(\sigma_{i}\left(YX^{T}\right)\right)^{\frac{m-k}{m}}\left(\sigma_{j}\left(YX^{T}\right)\right)^{\frac{k-1}{m}}\tilde{b}_{i.j}\right]^{2}\\
 & = & \max_{\left\Vert \tilde{B}\right\Vert _{F}=1}2\sum_{i,j=1}^{N}\tilde{b}_{i.j}^{2}\sum_{k=1}^{m}\left[\left(\sigma_{i}\left(YX^{T}\right)\right)^{\frac{m-k}{m}}\left(\sigma_{j}\left(YX^{T}\right)\right)^{\frac{k-1}{m}}\right]^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
This is a simple linear optimization problem over the unit simplex, whose
 optimal value is attained at the the vertex 
\begin_inset Formula $\tilde{B}=\left(\delta_{1,1}(i,j)\right)_{i=1,j=1}^{d_{x}\ ,d_{x}}$
\end_inset

, where 
\begin_inset Formula $\delta$
\end_inset

 is the Kronecker delta.
 Thus the canonical solution sharpness is:
\begin_inset Formula 
\[
\lambda_{\mathrm{\max}}\left(H_{w^{\ast}}\right)=2\sum_{k=1}^{m}\left[\left(\sigma_{\max}\left(YX^{T}\right)\right)^{\frac{m-k}{m}}\left(\sigma_{\max}\left(YX^{T}\right)\right)^{\frac{k-1}{m}}\right]^{2}=2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}
\]

\end_inset


\end_layout

\begin_layout Proof
And by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lemma:unif-lb"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we get the desired equality: 
\begin_inset Formula 
\[
2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}\le\rho(YX^{T})\le\lambda_{\mathrm{\max}}\left(H_{w^{\ast}}\right)=2m\left(\sigma_{\max}\left(YX^{T}\right)\right)^{2\left(1-\frac{1}{m}\right)}
\]

\end_inset


\end_layout

\begin_layout Subsection
Proof of Corollary 
\begin_inset CommandInset ref
LatexCommand ref
reference "cor:dead-direction-local"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\end_layout

\begin_layout Proof
We prove for the case of 
\begin_inset Formula $d_{y}\le d_{x}$
\end_inset

, the case where 
\begin_inset Formula $d_{x}\le d_{y}$
\end_inset

 is analogous.
\end_layout

\begin_layout Proof
Define 
\begin_inset Formula $\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)$
\end_inset

 by the point 
\begin_inset Formula $\left(\kaliseries\bar{W},m\right)$
\end_inset

 as in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "def:dead-direction-delta"
plural "false"
caps "false"
noprefix "false"

\end_inset

, by construction it holds that 
\begin_inset Formula $T_{2}=\left(\bar{W}_{i}+\Delta_{i}\right)_{1:m}$
\end_inset

.
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $t\in\mathbb{R}$
\end_inset

, for-each point 
\begin_inset Formula $(W_{1},W_{2},...,W_{m})\in B\left((U_{1},U_{2},...,U_{m}),\varepsilon\right)$
\end_inset

 we examine the loss along 
\begin_inset Formula $\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)$
\end_inset

:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{align*}
\phi\left((W_{1},W_{2},...,W_{m})+t\left(\Delta_{1},\Delta_{2},...,\Delta_{m}\right)\right) & = & \left\Vert \left(W_{i}+t\Delta_{i}\right)_{1:m}X-Y\right\Vert _{F}^{2}\\
 & = & \left\Vert W_{2:m}\left(W_{1}+t\left(\bar{W}_{2:m}\right)^{+}\left(T_{2}-T_{1}\right)\right)X-Y\right\Vert _{F}^{2}\\
 & = & \left\Vert W_{2:m}W_{1}X+tW_{2:m}\left(\bar{W}_{2:m}\right)^{+}{\color{gray}\underset{=0}{\underbrace{{\normalcolor \left(T_{2}-T_{1}\right)X}}}}-Y\right\Vert _{F}^{2}\\
 & = & \left\Vert W_{1:m}X-Y\right\Vert _{F}^{2}\\
 & = & \phi\left(\kaliseries\bar{W},m\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Proof
Therefore we conclude that:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{equation}
\exists\varepsilon>0.\ \phi|_{B\left((\bar{W}_{1},\bar{W}_{2},...,\bar{W}_{m}),\varepsilon\right)}\equiv\phi\circ s_{t}|_{B\left((\bar{W}_{1},\bar{W}_{2},...,\bar{W}_{m}),\varepsilon\right)}\label{eq:equvilent-neig-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:proof-of-ex-of-frs"

\end_inset

Proof of Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:existence-of-fr"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Proof
We show the case of 
\begin_inset Formula $d_{y}\le d_{x}$
\end_inset

.
 
\end_layout

\begin_layout Proof
Denote 
\begin_inset Formula $T_{0}=YX^{T}$
\end_inset

, and its SVD by 
\begin_inset Formula $USV^{T}$
\end_inset

 where 
\begin_inset Formula $r=rank\left(T_{0}\right)$
\end_inset

.
 If 
\begin_inset Formula $T_{0}$
\end_inset

is of full rank, then for any overparameterization 
\begin_inset Formula $\left(\kaliseries W,m\right)\in\Theta(T_{0})$
\end_inset

 it holds that 
\begin_inset Formula $rank\left(W_{m:2}\right)=r$
\end_inset

.
\end_layout

\begin_layout Proof
Otherwise, let 
\begin_inset Formula $\left(W_{1}^{\ast},W_{2}^{\ast},...,W_{m}^{\ast}\right)\in\varTheta(T_{0})$
\end_inset

 be its canonical overparameterization, defined in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:canon-singular"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Proof
We define 
\begin_inset Formula $(W_{1},W_{2},...,W_{m})\in\varTheta(T_{0})$
\end_inset

, by perturbing the canonical overparameterization into non-singularity:
\end_layout

\begin_layout Proof
Let 
\begin_inset Formula $0<\varepsilon\le\left(\sigma_{\max}\left(YX^{T}\right)\right)^{\frac{1}{m}}$
\end_inset

, define 
\begin_inset Formula $\tilde{S}\in\mathbb{R}^{d_{y}\times d_{x}}$
\end_inset

 by: 
\begin_inset Formula 
\[
\tilde{S}=S+diag\left({\color{gray}\underset{r\ times}{\underbrace{{\normalcolor 0,0,...,0}}}},{\color{gray}\underset{d_{y}-r\ times}{\underbrace{{\normalcolor \varepsilon,\varepsilon,...,\varepsilon}}}}\right)
\]

\end_inset

 and 
\begin_inset Formula $(W_{1},W_{2},...,W_{m})$
\end_inset

 by:
\begin_inset Formula 
\[
W_{m}=U\tilde{S}_{m}^{\frac{1}{m}},\quad W_{j}=\tilde{S}_{j}^{\frac{1}{m}}\quad j\in\left\{ 2,3,...,m-1\right\} ,\quad W_{1}=S_{1}^{\frac{1}{m}}V^{T}
\]

\end_inset


\end_layout

\begin_layout Proof
where 
\begin_inset Formula $S_{j}^{\frac{1}{m}}$
\end_inset

 is used to denote the 
\begin_inset Formula $d_{j}\times d_{j-1}$
\end_inset

 matrix whose 
\begin_inset Formula $k$
\end_inset

th diagonal entry is 
\begin_inset Formula $\left(\sigma_{k}\left(T_{0}\right)\right)^{\frac{1}{m}}$
\end_inset

.
 
\end_layout

\begin_layout Proof
Note that by construction 
\begin_inset Formula $rank\left(W_{2:m}\right)=d_{y}$
\end_inset

 and that 
\begin_inset Formula 
\[
W_{1}^{\ast}=S_{1}^{\frac{1}{m}}V^{T}=\begin{pmatrix}diag\left(\sigma_{1}\left(T_{0}\right),...,\sigma_{rank(S)}\left(T_{0}\right)\right) & \mathbb{0}_{rank(S)\times\left(d_{x}-rank(S)\right)}\\
\mathbb{0}_{\left(d_{1}-rank(S)\right)\times rank(S)} & \mathbb{0}_{\left(d_{1}-rank(S)\right)\times\left(d_{x}-rank(S)\right)}
\end{pmatrix}V^{T}
\]

\end_inset

 therefore:
\begin_inset Formula 
\[
W_{1:m}=U\tilde{S}^{\frac{m-1}{m}}S^{\frac{1}{m}}V^{T}=USV^{T}=T_{0}
\]

\end_inset


\end_layout

\begin_layout Proof
By 
\begin_inset CommandInset ref
LatexCommand eqref
reference "thm:sufficient-singular"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\begin_inset Formula $(W_{1},W_{2},...,W_{m})\in\Omega_{0}(T_{0})$
\end_inset

 and 
\begin_inset Formula $T_{0}\in\Psi_{0}$
\end_inset

as required.
\end_layout

\begin_layout Proof
The case of 
\begin_inset Formula $d_{x}\le d_{y}$
\end_inset

 is analogous.
\end_layout

\end_body
\end_document
